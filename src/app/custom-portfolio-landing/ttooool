[{"id":"llm_web_search","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"LLM Web Search","content":"\"\"\"\nLLM Web Search\nversion: 0.4.0\n\nCopyright (C) 2024 mamei16\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\n\nimport os\nfrom types import TracebackType\nfrom typing import (\n    Dict,\n    Tuple,\n    cast,\n    Any,\n    List,\n    Literal,\n    Optional,\n    Union,\n    Callable,\n    Iterable,\n    Sequence,\n    Iterator,\n)\nfrom dataclasses import dataclass\nimport urllib\nfrom urllib.parse import urlparse\nimport re\nimport warnings\nimport copy\nimport math\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom itertools import chain\nimport asyncio\nimport concurrent.futures\n\nfrom pydantic import BaseModel, Field\nimport aiohttp\nimport numpy as np\nfrom requests.exceptions import JSONDecodeError\nfrom bs4 import BeautifulSoup\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.sparse import csr_array\nimport torch\nfrom torch import Tensor\nfrom sentence_transformers import SentenceTransformer, quantize_embeddings\nfrom sentence_transformers.util import batch_to_device, truncate_embeddings\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom duckduckgo_search import DDGS\nfrom duckduckgo_search.utils import json_loads\nfrom duckduckgo_search.exceptions import DuckDuckGoSearchException\nfrom open_webui.env import BASE_DIR\n\ntry:\n    from dotenv import find_dotenv, load_dotenv\n\n    load_dotenv(find_dotenv(str(BASE_DIR / \".env\")))\nexcept ImportError:\n    print(\"dotenv not installed, skipping...\")\n\n\nclass AsyncDDGS(DDGS):\n    def __init__(\n        self,\n        headers: dict[str, str] | None = None,\n        proxy: str | None = None,\n        proxies: dict[str, str] | str | None = None,  # deprecated\n        timeout: int | None = 10,\n        verify: bool = True,\n    ) -> None:\n        \"\"\"Initialize the AsyncDDGS object.\n\n        Args:\n            headers (dict, optional): Dictionary of headers for the HTTP client. Defaults to None.\n            proxy (str, optional): proxy for the HTTP client, supports http/https/socks5 protocols.\n                example: \"http://user:pass@example.com:3128\". Defaults to None.\n            timeout (int, optional): Timeout value for the HTTP client. Defaults to 10.\n            verify (bool): SSL verification when making the request. Defaults to True.\n        \"\"\"\n        super().__init__(\n            headers=headers,\n            proxy=proxy,\n            proxies=proxies,\n            timeout=timeout,\n            verify=verify,\n        )\n        self._executor = concurrent.futures.ThreadPoolExecutor()\n        self._loop = asyncio.get_running_loop()\n\n    async def __aenter__(self) -> \"AsyncDDGS\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        pass\n\n    async def atext(\n        self,\n        keywords: str,\n        region: str = \"wt-wt\",\n        safesearch: str = \"moderate\",\n        timelimit: str | None = None,\n        backend: str = \"api\",\n        max_results: int | None = None,\n    ) -> list[dict[str, str]]:\n        result = await self._loop.run_in_executor(\n            self._executor,\n            super().text,\n            keywords,\n            region,\n            safesearch,\n            timelimit,\n            backend,\n            max_results,\n        )\n        return result\n\n    def answers(self, keywords: str) -> list[dict[str, str]]:\n        \"\"\"DuckDuckGo instant answers. Query params: https://duckduckgo.com/params.\n\n        Args:\n            keywords: keywords for query,\n\n        Returns:\n            List of dictionaries with instant answers results.\n\n        Raises:\n            DuckDuckGoSearchException: Base exception for duckduckgo_search errors.\n            RatelimitException: Inherits from DuckDuckGoSearchException, raised for exceeding API request rate limits.\n            TimeoutException: Inherits from DuckDuckGoSearchException, raised for API request timeouts.\n        \"\"\"\n        assert keywords, \"keywords is mandatory\"\n\n        payload = {\n            \"q\": f\"what is {keywords}\",\n            \"format\": \"json\",\n        }\n        try:\n            resp_content = self._get_url(\n                \"GET\", \"https://api.duckduckgo.com/\", params=payload\n            )\n            if not isinstance(resp_content, bytes) and hasattr(resp_content, \"content\"):\n                resp_content = resp_content.content\n            page_data = json_loads(resp_content)\n        except DuckDuckGoSearchException as e:\n            print(f\"LLM_Web_search | DuckDuckGo instant answer yielded error: {str(e)}\")\n            return []\n\n        results = []\n        answer = page_data.get(\"AbstractText\")\n        url = page_data.get(\"AbstractURL\")\n        if answer:\n            results.append(\n                {\n                    \"icon\": None,\n                    \"text\": answer,\n                    \"topic\": None,\n                    \"url\": url,\n                }\n            )\n\n        return results\n\n    async def aanswers(\n        self,\n        keywords: str,\n    ) -> list[dict[str, str]]:\n        \"\"\"DuckDuckGo async instant answers. Query params: https://duckduckgo.com/params.\n\n        Args:\n            keywords: keywords for query,\n\n        Returns:\n            List of dictionaries with instant answers results.\n\n        Raises:\n            DuckDuckGoSearchException: Base exception for duckduckgo_search errors.\n            RatelimitException: Inherits from DuckDuckGoSearchException, raised for exceeding API request rate limits.\n            TimeoutException: Inherits from DuckDuckGoSearchException, raised for API request timeouts.\n        \"\"\"\n        result = await self._loop.run_in_executor(\n            self._executor,\n            self.answers,\n            keywords,\n        )\n        return result\n\n\nasync def emit_status(event_emitter, description: str, done: bool):\n    if event_emitter:\n        await event_emitter(\n            {\n                \"type\": \"status\",\n                \"data\": {\n                    \"description\": description,\n                    \"done\": done,\n                },\n            }\n        )\n\n\nasync def emit_message(event_emitter, content: str):\n    if event_emitter:\n        await event_emitter(\n            {\n                \"type\": \"message\",\n                \"data\": {\"content\": content},\n            }\n        )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        embedding_model_save_path: str = Field(\n            default=\"\",\n            description=\"Path to the folder in which embedding models will be saved\",\n        )\n        num_results: int = Field(\n            default=10,\n            description=\"Number of search engine results to process per query\",\n            ge=1,\n        )\n        max_results: int = Field(\n            default=8,\n            description=\"Max. number of search results to return per query\",\n            ge=1,\n        )\n        cpu_only: bool = Field(default=False, description=\"Run the tool on CPU only\")\n        simple_search: bool = Field(\n            default=False,\n            description=\"Use just the website snippets returned by the search engine, instead of processing entire webpages\",\n        )\n        keep_results_in_context: bool = Field(\n            default=True,\n            description=\"Keep search results in context. This allows the model to re-use previous search results for follow-up questions,\"\n            \"but uses more VRAM and will slow down responses as the results accumulate.\",\n        )\n        chunk_size: int = Field(\n            default=500,\n            description=\"Max. chunk size. The maximal size of the individual chunks that each webpage will\"\n            \" be split into, in characters\",\n            ge=5,\n            le=100000,\n        )\n        include_citations: bool = Field(\n            default=True,\n            description=\"Include a citation for each retrieved search result\",\n        )\n        ensemble_weighting: float = Field(\n            default=0.5,\n            description=\"Ensemble Weighting. \"\n            \"Smaller values = More keyword oriented, Larger values = More focus on semantic similarity\",\n            ge=0.0,\n            le=1.0,\n        )\n        keyword_retriever: str = Field(\n            default=\"splade\",\n            description=\"Keyword retriever. Must be either 'bm25' or 'splade'.\",\n            pattern=r\"^(bm25|splade)$\",\n        )\n        splade_batch_size: int = Field(\n            default=8,\n            description=\"SPLADE batch size. Smaller values = Slower retrieval (but lower VRAM usage), \"\n            \"Larger values = Faster retrieval (but higher VRAM usage).\",\n            ge=2,\n            le=1024,\n        )\n        chunker: str = Field(\n            default=\"semantic\",\n            description=\"Chunking method. Must be either 'character-based' or 'semantic'.\",\n            pattern=r\"^(character-based|semantic)$\",\n        )\n        chunker_breakpoint_threshold_amount: int = Field(\n            default=30,\n            description=\"Semantic chunking: sentence split threshold (%).\"\n            \"Defines how different two consecutive sentences have\"\n            \" to be for them to be split into separate chunks\",\n            ge=1,\n            le=100,\n        )\n        similarity_score_threshold: float = Field(\n            default=0.5,\n            description=\"Similarity Score Threshold. \"\n            \"Discard chunks that are not similar enough to the \"\n            \"search query and hence fall below the threshold.\",\n            ge=0.0,\n            le=1.0,\n        )\n        client_timeout: int = Field(\n            default=10,\n            description=\"Client timeout (in seconds).\"\n            \"When reached, pending or unfinished webpage \"\n            \"downloads will be cancelled to start the retrieval process immediately\",\n            ge=0,\n            le=1000,\n        )\n        searxng_url: str = Field(\n            default=\"None\",\n            description='SearXNG URL. If not equal to \"None\", searXNG will be used instead of DuckDuckGo',\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.document_retriever = DocumentRetriever()\n\n    @staticmethod\n    def reuse_existing_web_search_results(__user__: dict, __event_emitter__=None):\n        \"\"\"\n        Choose this tool if existing search results from a previous web search can be used to answer the user's query.\n        \"\"\"\n        return \"\"\n\n    @staticmethod\n    def no_tool_necessary(__user__: dict, __event_emitter__=None):\n        \"\"\"\n        Choose this tool if you can answer the user without using any tool.\n        \"\"\"\n        return \"\"\n\n    async def search_web(\n        self, query: str, __user__: dict, __event_emitter__=None\n    ) -> str:\n        \"\"\"\n        The search tool will search the web and return the results. You must formulate your own search query based on the user's message.\n        \"\"\"\n        self.document_retriever.update_settings(self.valves)\n\n        if self.valves.embedding_model_save_path == \"\":\n            await emit_status(\n                __event_emitter__,\n                \"Error: Please configure the embedding model save path\",\n                True,\n            )\n            error_message = (\n                \"Error: Please configure the embedding model save path. \"\n                \"To solve this issue, go to Workspace-->Tools and click on the gear symbol next to the LLM_Web_search tool. \"\n                'Then, fill out the field titled \"Embedding Model Save Path\" with the absolute path to the directory '\n                \"in which the embedding models should be stored.\"\n            )\n            await emit_message(__event_emitter__, f\"\\[ % {error_message}\\n \\] \")\n            return error_message\n\n        try:\n            if (\n                self.document_retriever.splade_doc_model is None\n                or self.document_retriever.splade_query_model is None\n                or self.document_retriever.embedding_model is None\n            ):\n                await self.document_retriever.aload_models(__event_emitter__)\n\n            if self.valves.searxng_url != \"None\":\n                result_docs = await self.document_retriever.aretrieve_from_searxng(\n                    query, self.valves.simple_search, __event_emitter__\n                )\n            else:\n                result_docs = await self.document_retriever.aretrieve_from_duckduckgo(\n                    query, self.valves.simple_search, __event_emitter__\n                )\n            source_url_set = list({d.metadata[\"source\"] for d in result_docs})\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"action\": \"web_search\",\n                            \"description\": f\"Web search retrieved {len(result_docs)} results from {len(source_url_set)} sources\",\n                            \"done\": True,\n                            \"query\": query,\n                            \"urls\": source_url_set,\n                        },\n                    }\n                )\n\n            if self.valves.include_citations and __event_emitter__:\n                for result_doc in result_docs:\n                    source = result_doc.metadata[\"source\"]\n                    if source != \"SearXNG instant answer\":\n                        source = urlparse(source).netloc.lstrip(\"www.\")\n                    await __event_emitter__(\n                        {\n                            \"type\": \"citation\",\n                            \"data\": {\n                                \"document\": [result_doc.page_content],\n                                \"metadata\": [result_doc.metadata],\n                                \"source\": {\"name\": source},\n                            },\n                        }\n                    )\n\n            pretty_docs_string = docs_to_pretty_str(result_docs)\n            if self.valves.keep_results_in_context:\n                escaped_docs_string = katex_escape_str(pretty_docs_string)\n                await emit_message(\n                    __event_emitter__, f\"\\\\[ % {escaped_docs_string}\\n \\\\] \"\n                )\n            return pretty_docs_string\n        except Exception as exc:\n            exception_message = str(exc)\n            await emit_status(\n                __event_emitter__,\n                f\"The search tool encountered an error: {exception_message}\",\n                True,\n            )\n            return f\"The search tool encountered an error: {exception_message}\"\n\n\ndef katex_escape_str(string: str) -> str:\n    return (\n        string.replace(\"\\n\", \"\\\\n\")\n        .replace(\"\\\\[\", \"{[}\")\n        .replace(\"\\\\]\", \"{]}\")\n        .replace(\"\\r\", \"\")\n    )\n\n\ndef load_splade_model(repo_id: str, cache_dir: str, device: str):\n    kwargs = {\n        \"cache_dir\": cache_dir,\n        \"torch_dtype\": torch.float32 if device == \"cpu\" else torch.float16,\n        \"attn_implementation\": \"eager\",\n    }\n    try:\n        return AutoTokenizer.from_pretrained(\n            repo_id, cache_dir=cache_dir\n        ), AutoModelForMaskedLM.from_pretrained(\n            repo_id, local_files_only=True, **kwargs\n        )\n    except OSError:\n        return AutoTokenizer.from_pretrained(\n            repo_id, cache_dir=cache_dir\n        ), AutoModelForMaskedLM.from_pretrained(repo_id, **kwargs)\n\n\ndef load_embedding_model(repo_id: str, cache_dir: str, device: str):\n    return MySentenceTransformer(\n        repo_id,\n        cache_folder=cache_dir,\n        device=device,\n        model_kwargs={\n            \"torch_dtype\": torch.float32 if device == \"cpu\" else torch.float16\n        },\n    )\n\n\n@dataclass\nclass Document:\n    page_content: str\n    metadata: Dict\n\n\nclass DocumentRetriever:\n    spaces_regex: re.Pattern\n    device: str\n    model_cache_dir: str\n    num_results: int\n    max_results: int\n    similarity_threshold: float\n    keyword_retriever: str\n    chunking_method: str\n    chunk_size: int\n    chunker_breakpoint_threshold_amount: int\n    ensemble_weighting: float\n    client_timeout: int\n    searxng_url: str\n    splade_batch_size: int\n\n    def __init__(self):\n        self.embedding_model = None\n        self.splade_doc_tokenizer = None\n        self.splade_doc_model = None\n        self.splade_query_tokenizer = None\n        self.splade_query_model = None\n        self.spaces_regex = re.compile(r\" {3,}\")\n        self.proxy = None\n        self.proxy_except_domains = None\n\n    def update_settings(self, settings: Tools.Valves):\n        self.device = \"cpu\" if settings.cpu_only else \"cuda\"\n        self.model_cache_dir = settings.embedding_model_save_path\n        self.num_results = settings.num_results\n        self.max_results = settings.max_results\n        self.similarity_threshold = settings.similarity_score_threshold\n        self.keyword_retriever = settings.keyword_retriever\n        self.chunking_method = settings.chunker\n        self.chunk_size = settings.chunk_size\n        self.chunker_breakpoint_threshold_amount = (\n            settings.chunker_breakpoint_threshold_amount\n        )\n        self.ensemble_weighting = settings.ensemble_weighting\n        self.client_timeout = settings.client_timeout\n        self.searxng_url = settings.searxng_url\n        self.splade_batch_size = settings.splade_batch_size\n        self.proxy = os.environ.get(\"https_proxy\", os.environ.get(\"http_proxy\"))\n        if os.environ.get(\"no_proxy\"):\n            self.proxy_except_domains = tuple(os.environ.get(\"no_proxy\").split(\",\"))\n\n    async def aload_models(self, __event_emitter__):\n        await emit_status(__event_emitter__, \"Loading embedding model 1/3...\", False)\n\n        self.embedding_model = await asyncio.to_thread(\n            load_embedding_model, \"all-MiniLM-L6-v2\", self.model_cache_dir, self.device\n        )\n        self.embedding_model.to(self.device)\n\n        await emit_status(__event_emitter__, \"Loading embedding model 2/3...\", False)\n        self.splade_doc_tokenizer, self.splade_doc_model = await asyncio.to_thread(\n            load_splade_model,\n            \"naver/efficient-splade-VI-BT-large-doc\",\n            self.model_cache_dir,\n            self.device,\n        )\n        self.splade_doc_model.to(self.device)\n\n        await emit_status(__event_emitter__, \"Loading embedding model 3/3...\", False)\n        self.splade_query_tokenizer, self.splade_query_model = await asyncio.to_thread(\n            load_splade_model,\n            \"naver/efficient-splade-VI-BT-large-query\",\n            self.model_cache_dir,\n            self.device,\n        )\n        self.splade_query_model.to(self.device)\n\n    async def aretrieve_from_duckduckgo(\n        self, query: str, simple_search: bool, event_emitter\n    ):\n        documents = []\n        query = query.strip(\"\\\"'\")\n        max_results = self.max_results\n        await emit_status(\n            event_emitter, f'Searching DuckDuckGo for \"{query}\"...', False\n        )\n\n        with AsyncDDGS(proxy=self.proxy) as ddgs:\n            answer_list = await ddgs.aanswers(query)\n            if answer_list:\n                if max_results > 1:\n                    max_results -= 1  # We already have 1 result now\n                answer_dict = answer_list[0]\n                instant_answer_doc = Document(\n                    page_content=answer_dict[\"text\"],\n                    metadata={\"source\": answer_dict[\"url\"]},\n                )\n                documents.append(instant_answer_doc)\n\n            result_documents = []\n            result_urls = []\n            for result in await ddgs.atext(\n                query,\n                region=\"wt-wt\",\n                safesearch=\"moderate\",\n                timelimit=None,\n                max_results=self.num_results,\n            ):\n                result_document = Document(\n                    page_content=f\"Title: {result['title']}\\n{result['body']}\",\n                    metadata={\"source\": result[\"href\"]},\n                )\n                result_documents.append(result_document)\n                result_urls.append(result[\"href\"])\n\n        if simple_search:\n            retrieved_docs = await self.aretrieve_from_snippets(\n                query, result_documents, event_emitter\n            )\n        else:\n            retrieved_docs = await self.aretrieve_from_webpages(\n                query, result_urls, event_emitter\n            )\n\n        documents.extend(retrieved_docs)\n\n        if (\n            not documents\n        ):  # Fall back to old simple search rather than returning nothing\n            print(\n                \"LLM_Web_search | Could not find any page content \"\n                \"similar enough to be extracted, using basic search fallback...\"\n            )\n            return result_documents[:max_results]\n        return documents[:max_results]\n\n    async def aretrieve_from_searxng(\n        self, query: str, simple_search: bool, event_emitter\n    ):\n        await emit_status(event_emitter, f'Searching SearXNG for \"{query}\"...', False)\n\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n            \"Accept-Language\": \"en-US,en;q=0.5\",\n        }\n        result_documents = []\n        result_urls = []\n        request_str = f\"/search?q={urllib.parse.quote(query)}&format=json&pageno=\"\n        pageno = 1\n        url = (\n            self.searxng_url\n            if self.searxng_url.startswith(\"http\")\n            else (\"http://\" + self.searxng_url)\n        )\n        async with aiohttp.ClientSession(headers=headers) as session:\n            while len(result_urls) < self.num_results:\n                response = await session.get(url + request_str + str(pageno))\n\n                if not result_urls:  # no results to lose by raising an exception here\n                    response.raise_for_status()\n                try:\n                    response_dict = await response.json()\n                except JSONDecodeError:\n                    raise ValueError(\n                        \"JSONDecodeError: Please ensure that the SearXNG instance can return data in JSON format\"\n                    )\n\n                result_dicts = response_dict[\"results\"]\n                if not result_dicts:\n                    break\n                for result in result_dicts:\n                    if (\n                        \"content\" in result\n                    ):  # Since some websites don't provide any description\n                        result_document = Document(\n                            page_content=f\"Title: {result['title']}\\n{result['content']}\",\n                            metadata={\"source\": result[\"url\"]},\n                        )\n                        result_documents.append(result_document)\n                    result_urls.append(result[\"url\"])\n\n                answers = response_dict[\"answers\"]\n                for answer in answers:\n                    answer_document = Document(\n                        page_content=f\"Title: {query}\\n{answer}\",\n                        metadata={\"source\": \"SearXNG instant answer\"},\n                    )\n                    result_documents.append(answer_document)\n                pageno += 1\n\n        if simple_search:\n            retrieved_docs = await self.aretrieve_from_snippets(\n                query, result_documents, event_emitter\n            )\n        else:\n            retrieved_docs = await self.aretrieve_from_webpages(\n                query, result_urls, event_emitter\n            )\n\n        return retrieved_docs[: self.max_results]\n\n    def preprocess_text(self, text: str) -> str:\n        text = text.replace(\"\\n\", \" \\n\")\n        text = self.spaces_regex.sub(\" \", text)\n        text = text.strip()\n        return text\n\n    async def aretrieve_from_snippets(\n        self, query: str, documents: list[Document], event_emitter\n    ) -> list[Document]:\n        await emit_status(event_emitter, \"Retrieving relevant results...\", False)\n\n        dense_retriever = DenseRetriever(\n            self.embedding_model,\n            num_results=self.num_results,\n            similarity_threshold=self.similarity_threshold,\n        )\n        dense_retriever.add_documents(documents)\n        return dense_retriever.get_relevant_documents(query)\n\n    async def aretrieve_from_webpages(\n        self, query: str, url_list: list[str], event_emitter\n    ) -> list[Document]:\n        if self.chunking_method == \"semantic\":\n            text_splitter = BoundedSemanticChunker(\n                self.embedding_model,\n                breakpoint_threshold_type=\"percentile\",\n                breakpoint_threshold_amount=self.chunker_breakpoint_threshold_amount,\n                max_chunk_size=self.chunk_size,\n            )\n        else:\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=self.chunk_size,\n                chunk_overlap=10,\n                separators=[\"\\n\\n\", \"\\n\", \".\", \", \", \" \", \"\"],\n            )\n\n        await emit_status(event_emitter, \"Downloading and chunking webpages...\", False)\n        split_docs = await async_fetch_chunk_websites(\n            url_list,\n            text_splitter,\n            self.client_timeout,\n            self.proxy,\n            self.proxy_except_domains,\n        )\n\n        await emit_status(event_emitter, \"Retrieving relevant results...\", False)\n        if self.ensemble_weighting > 0:\n            dense_retriever = DenseRetriever(\n                self.embedding_model,\n                num_results=self.num_results,\n                similarity_threshold=self.similarity_threshold,\n            )\n            dense_retriever.add_documents(split_docs)\n            dense_result_docs = dense_retriever.get_relevant_documents(query)\n        else:\n            dense_result_docs = []\n\n        if self.ensemble_weighting < 1:\n            #  The sparse keyword retriever is good at finding relevant documents based on keywords,\n            #  while the dense retriever is good at finding relevant documents based on semantic similarity.\n            if self.keyword_retriever == \"bm25\":\n                keyword_retriever = BM25Retriever.from_documents(\n                    split_docs, preprocess_func=self.preprocess_text\n                )\n                keyword_retriever.k = self.num_results\n            elif self.keyword_retriever == \"splade\":\n                keyword_retriever = SpladeRetriever(\n                    splade_doc_tokenizer=self.splade_doc_tokenizer,\n                    splade_doc_model=self.splade_doc_model,\n                    splade_query_tokenizer=self.splade_query_tokenizer,\n                    splade_query_model=self.splade_query_model,\n                    device=self.device,\n                    batch_size=self.splade_batch_size,\n                    k=self.num_results,\n                )\n                await asyncio.to_thread(keyword_retriever.add_documents, split_docs)\n            else:\n                raise ValueError(\n                    \"self.keyword_retriever must be one of ('bm25', 'splade')\"\n                )\n            sparse_results_docs = await asyncio.to_thread(\n                keyword_retriever.get_relevant_documents, query\n            )\n        else:\n            sparse_results_docs = []\n\n        return weighted_reciprocal_rank(\n            [dense_result_docs, sparse_results_docs],\n            weights=[self.ensemble_weighting, 1 - self.ensemble_weighting],\n        )[: self.num_results]\n\n\ndef cosine_similarity(X, Y) -> np.ndarray:\n    \"\"\"Row-wise cosine similarity between two equal-width matrices.\"\"\"\n    if len(X) == 0 or len(Y) == 0:\n        return np.array([])\n\n    X = np.array(X)\n    Y = np.array(Y)\n    if X.shape[1] != Y.shape[1]:\n        raise ValueError(\n            f\"Number of columns in X and Y must be the same. X has shape {X.shape} \"\n            f\"and Y has shape {Y.shape}.\"\n        )\n    X_norm = np.linalg.norm(X, axis=1)\n    Y_norm = np.linalg.norm(Y, axis=1)\n    # Ignore divide by zero errors run time warnings as those are handled below.\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        similarity = np.dot(X, Y.T) / np.outer(X_norm, Y_norm)\n    similarity[np.isnan(similarity) | np.isinf(similarity)] = 0.0\n    return similarity\n\n\ndef dict_list_to_pretty_str(data: list[dict]) -> str:\n    ret_str = \"\"\n    if isinstance(data, dict):\n        data = [data]\n    if isinstance(data, list):\n        for i, d in enumerate(data):\n            ret_str += f\"Result {i + 1}\\n\"\n            ret_str += f\"Title: {d['title']}\\n\"\n            ret_str += f\"{d['body']}\\n\"\n            ret_str += f\"Source URL: {d['href']}\\n\"\n        return ret_str\n    else:\n        raise ValueError(\"Input must be dict or list[dict]\")\n\n\nclass TextSplitter:\n    \"\"\"Interface for splitting text into chunks.\n    Source: https://github.com/langchain-ai/langchain/blob/master/libs/text-splitters/langchain_text_splitters/base.py#L30\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        length_function: Callable[[str], int] = len,\n        keep_separator: Union[bool, Literal[\"start\", \"end\"]] = False,\n        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\n\n        Args:\n            chunk_size: Maximum size of chunks to return\n            chunk_overlap: Overlap in characters between chunks\n            length_function: Function that measures the length of given chunks\n            keep_separator: Whether to keep the separator and where to place it\n                            in each corresponding chunk (True='start')\n            add_start_index: If `True`, includes chunk's start index in metadata\n            strip_whitespace: If `True`, strips whitespace from the start and end of\n                              every document\n        \"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self._length_function = length_function\n        self._keep_separator = keep_separator\n        self._add_start_index = add_start_index\n        self._strip_whitespace = strip_whitespace\n\n    @abstractmethod\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into multiple components.\"\"\"\n\n    def create_documents(\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\n    ) -> List[Document]:\n        \"\"\"Create documents from a list of texts.\"\"\"\n        _metadatas = metadatas or [{}] * len(texts)\n        documents = []\n        for i, text in enumerate(texts):\n            index = 0\n            previous_chunk_len = 0\n            for chunk in self.split_text(text):\n                metadata = copy.deepcopy(_metadatas[i])\n                if self._add_start_index:\n                    offset = index + previous_chunk_len - self._chunk_overlap\n                    index = text.find(chunk, max(0, offset))\n                    metadata[\"start_index\"] = index\n                    previous_chunk_len = len(chunk)\n                new_doc = Document(page_content=chunk, metadata=metadata)\n                documents.append(new_doc)\n        return documents\n\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\n        \"\"\"Split documents.\"\"\"\n        texts, metadatas = [], []\n        for doc in documents:\n            texts.append(doc.page_content)\n            metadatas.append(doc.metadata)\n        return self.create_documents(texts, metadatas=metadatas)\n\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n        text = separator.join(docs)\n        if self._strip_whitespace:\n            text = text.strip()\n        if text == \"\":\n            return None\n        else:\n            return text\n\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        separator_len = self._length_function(separator)\n\n        docs = []\n        current_doc: List[str] = []\n        total = 0\n        for d in splits:\n            _len = self._length_function(d)\n            if (\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\n                > self._chunk_size\n            ):\n                if total > self._chunk_size:\n                    warnings.warn(\n                        f\"Created a chunk of size {total}, \"\n                        f\"which is longer than the specified {self._chunk_size}\"\n                    )\n                if len(current_doc) > 0:\n                    doc = self._join_docs(current_doc, separator)\n                    if doc is not None:\n                        docs.append(doc)\n                    # Keep on popping if:\n                    # - we have a larger chunk than in the chunk overlap\n                    # - or if we still have any chunks and the length is long\n                    while total > self._chunk_overlap or (\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n                        > self._chunk_size\n                        and total > 0\n                    ):\n                        total -= self._length_function(current_doc[0]) + (\n                            separator_len if len(current_doc) > 1 else 0\n                        )\n                        current_doc = current_doc[1:]\n            current_doc.append(d)\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\n        doc = self._join_docs(current_doc, separator)\n        if doc is not None:\n            docs.append(doc)\n        return docs\n\n    def transform_documents(\n        self, documents: Sequence[Document], **kwargs: Any\n    ) -> Sequence[Document]:\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\n        return self.split_documents(list(documents))\n\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    \"\"\"Splitting text by recursively look at characters.\n\n    Recursively tries to split by different characters to find one\n    that works.\n\n    Adapted from Langchain:\n    https://github.com/langchain-ai/langchain/blob/0606aabfa39acb2ec575ea8bbfa4c8e662a6134f/libs/text-splitters/langchain_text_splitters/character.py#L58\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        length_function: Callable[[str], int] = len,\n        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n        separators: Optional[List[str]] = None,\n        keep_separator: Union[bool, Literal[\"start\", \"end\"]] = True,\n        is_separator_regex: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(\n            chunk_size,\n            chunk_overlap,\n            length_function,\n            keep_separator,\n            add_start_index,\n            strip_whitespace,\n        )\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n\n        self._separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n        self._is_separator_regex = is_separator_regex\n\n    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n        \"\"\"Split incoming text and return chunks.\"\"\"\n        final_chunks = []\n        # Get appropriate separator to use\n        separator = separators[-1]\n        new_separators = []\n        for i, _s in enumerate(separators):\n            _separator = _s if self._is_separator_regex else re.escape(_s)\n            if _s == \"\":\n                separator = _s\n                break\n            if re.search(_separator, text):\n                separator = _s\n                new_separators = separators[i + 1 :]\n                break\n\n        _separator = separator if self._is_separator_regex else re.escape(separator)\n        splits = _split_text_with_regex(text, _separator, self._keep_separator)\n\n        # Now go merging things, recursively splitting longer texts.\n        _good_splits = []\n        _separator = \"\" if self._keep_separator else separator\n        for s in splits:\n            if self._length_function(s) < self._chunk_size:\n                _good_splits.append(s)\n            else:\n                if _good_splits:\n                    merged_text = self._merge_splits(_good_splits, _separator)\n                    final_chunks.extend(merged_text)\n                    _good_splits = []\n                if not new_separators:\n                    final_chunks.append(s)\n                else:\n                    other_info = self._split_text(s, new_separators)\n                    final_chunks.extend(other_info)\n        if _good_splits:\n            merged_text = self._merge_splits(_good_splits, _separator)\n            final_chunks.extend(merged_text)\n        return final_chunks\n\n    def split_text(self, text: str) -> List[str]:\n        return self._split_text(text, self._separators)\n\n\ndef _split_text_with_regex(\n    text: str, separator: str, keep_separator: Union[bool, Literal[\"start\", \"end\"]]\n) -> List[str]:\n    # Now that we have the separator, split the text\n    if separator:\n        if keep_separator:\n            # The parentheses in the pattern keep the delimiters in the result.\n            _splits = re.split(f\"({separator})\", text)\n            splits = (\n                ([_splits[i] + _splits[i + 1] for i in range(0, len(_splits) - 1, 2)])\n                if keep_separator == \"end\"\n                else ([_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)])\n            )\n            if len(_splits) % 2 == 0:\n                splits += _splits[-1:]\n            splits = (\n                (splits + [_splits[-1]])\n                if keep_separator == \"end\"\n                else ([_splits[0]] + splits)\n            )\n        else:\n            splits = re.split(separator, text)\n    else:\n        splits = list(text)\n    return [s for s in splits if s != \"\"]\n\n\ndef calculate_cosine_distances(sentence_embeddings) -> np.array:\n    \"\"\"Calculate cosine distances between sentences.\n\n    Args:\n        sentence_embeddings: List of sentence embeddings to calculate distances for.\n\n    Returns:\n        Distance between each pair of adjacent sentences\n    \"\"\"\n    # Sliding window array over each pair of adjacent sentence embeddings\n    sliding_windows = np.lib.stride_tricks.sliding_window_view(\n        sentence_embeddings, 2, axis=0\n    )\n\n    dot_prod = np.prod(sliding_windows, axis=-1).sum(axis=1)\n\n    magnitude_prod = np.prod(np.linalg.norm(sliding_windows, axis=1), axis=1)\n\n    cos_sim = dot_prod / magnitude_prod\n    return 1 - cos_sim\n\n\nBreakpointThresholdType = Literal[\"percentile\", \"standard_deviation\", \"interquartile\"]\nBREAKPOINT_DEFAULTS: Dict[BreakpointThresholdType, float] = {\n    \"percentile\": 95,\n    \"standard_deviation\": 3,\n    \"interquartile\": 1.5,\n}\n\n\nclass BoundedSemanticChunker(TextSplitter):\n    \"\"\"First splits the text using semantic chunking according to the specified\n    'breakpoint_threshold_amount', but then uses a RecursiveCharacterTextSplitter\n    to split all chunks that are larger than 'max_chunk_size'.\n\n    Adapted from langchain_experimental.text_splitter.SemanticChunker\"\"\"\n\n    def __init__(\n        self,\n        embedding_model: SentenceTransformer,\n        buffer_size: int = 1,\n        add_start_index: bool = False,\n        breakpoint_threshold_type: BreakpointThresholdType = \"percentile\",\n        breakpoint_threshold_amount: Optional[float] = None,\n        number_of_chunks: Optional[int] = None,\n        max_chunk_size: int = 500,\n        min_chunk_size: int = 4,\n    ):\n        super().__init__(add_start_index=add_start_index)\n        self._add_start_index = add_start_index\n        self.embedding_model = embedding_model\n        self.buffer_size = buffer_size\n        self.breakpoint_threshold_type = breakpoint_threshold_type\n        self.number_of_chunks = number_of_chunks\n        if breakpoint_threshold_amount is None:\n            self.breakpoint_threshold_amount = BREAKPOINT_DEFAULTS[\n                breakpoint_threshold_type\n            ]\n        else:\n            self.breakpoint_threshold_amount = breakpoint_threshold_amount\n        self.max_chunk_size = max_chunk_size\n        self.min_chunk_size = min_chunk_size\n        # Splitting the text on '.', '?', and '!'\n        self.sentence_split_regex = re.compile(r\"(?<=[.?!])\\s+\")\n\n        assert (\n            self.breakpoint_threshold_type == \"percentile\"\n        ), \"only breakpoint_threshold_type 'percentile' is currently supported\"\n        assert self.buffer_size == 1, \"combining sentences is not supported yet\"\n\n    def _calculate_sentence_distances(\n        self, sentences: List[dict]\n    ) -> Tuple[List[float], List[dict]]:\n        \"\"\"Split text into multiple components.\"\"\"\n        sentences = list(map(lambda x: x.replace(\"\\n\", \" \"), sentences))\n        embeddings = self.embedding_model.encode(sentences)\n        return calculate_cosine_distances(embeddings.tolist())\n\n    def _calculate_breakpoint_threshold(\n        self, distances: np.array, alt_breakpoint_threshold_amount=None\n    ) -> float:\n        if alt_breakpoint_threshold_amount is None:\n            breakpoint_threshold_amount = self.breakpoint_threshold_amount\n        else:\n            breakpoint_threshold_amount = alt_breakpoint_threshold_amount\n        if self.breakpoint_threshold_type == \"percentile\":\n            return cast(\n                float,\n                np.percentile(distances, breakpoint_threshold_amount),\n            )\n        elif self.breakpoint_threshold_type == \"standard_deviation\":\n            return cast(\n                float,\n                np.mean(distances) + breakpoint_threshold_amount * np.std(distances),\n            )\n        elif self.breakpoint_threshold_type == \"interquartile\":\n            q1, q3 = np.percentile(distances, [25, 75])\n            iqr = q3 - q1\n\n            return np.mean(distances) + breakpoint_threshold_amount * iqr\n        else:\n            raise ValueError(\n                f\"Got unexpected `breakpoint_threshold_type`: \"\n                f\"{self.breakpoint_threshold_type}\"\n            )\n\n    def _threshold_from_clusters(self, distances: List[float]) -> float:\n        \"\"\"\n        Calculate the threshold based on the number of chunks.\n        Inverse of percentile method.\n        \"\"\"\n        if self.number_of_chunks is None:\n            raise ValueError(\n                \"This should never be called if `number_of_chunks` is None.\"\n            )\n        x1, y1 = len(distances), 0.0\n        x2, y2 = 1.0, 100.0\n\n        x = max(min(self.number_of_chunks, x1), x2)\n\n        # Linear interpolation formula\n        y = y1 + ((y2 - y1) / (x2 - x1)) * (x - x1)\n        y = min(max(y, 0), 100)\n\n        return cast(float, np.percentile(distances, y))\n\n    def split_text(\n        self,\n        text: str,\n    ) -> List[str]:\n        sentences = self.sentence_split_regex.split(text)\n\n        # having len(sentences) == 1 would cause the following\n        # np.percentile to fail.\n        if len(sentences) == 1:\n            return sentences\n\n        bad_sentences = []\n\n        distances = self._calculate_sentence_distances(sentences)\n\n        if self.number_of_chunks is not None:\n            breakpoint_distance_threshold = self._threshold_from_clusters(distances)\n        else:\n            breakpoint_distance_threshold = self._calculate_breakpoint_threshold(\n                distances\n            )\n\n        indices_above_thresh = [\n            i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n        ]\n\n        chunks = []\n        start_index = 0\n\n        # Iterate through the breakpoints to slice the sentences\n        for index in indices_above_thresh:\n            # The end index is the current breakpoint\n            end_index = index\n\n            # Slice the sentence_dicts from the current start index to the end index\n            group = sentences[start_index : end_index + 1]\n            combined_text = \" \".join(group)\n            if self.min_chunk_size <= len(combined_text) <= self.max_chunk_size:\n                chunks.append(combined_text)\n            else:\n                sent_lengths = np.array([len(sd) for sd in group])\n                good_indices = np.flatnonzero(\n                    np.cumsum(sent_lengths) <= self.max_chunk_size\n                )\n                smaller_group = [group[i] for i in good_indices]\n                if smaller_group:\n                    combined_text = \" \".join(smaller_group)\n                    if len(combined_text) >= self.min_chunk_size:\n                        chunks.append(combined_text)\n                        group = group[good_indices[-1] :]\n                bad_sentences.extend(group)\n\n            # Update the start index for the next group\n            start_index = index + 1\n\n        # The last group, if any sentences remain\n        if start_index < len(sentences):\n            group = sentences[start_index:]\n            combined_text = \" \".join(group)\n            if self.min_chunk_size <= len(combined_text) <= self.max_chunk_size:\n                chunks.append(combined_text)\n            else:\n                sent_lengths = np.array([len(sd) for sd in group])\n                good_indices = np.flatnonzero(\n                    np.cumsum(sent_lengths) <= self.max_chunk_size\n                )\n                smaller_group = [group[i] for i in good_indices]\n                if smaller_group:\n                    combined_text = \" \".join(smaller_group)\n                    if len(combined_text) >= self.min_chunk_size:\n                        chunks.append(combined_text)\n                        group = group[good_indices[-1] :]\n                bad_sentences.extend(group)\n\n        # If pure semantic chunking wasn't able to split all text,\n        # split the remaining problematic text using a recursive character splitter instead\n        if len(bad_sentences) > 0:\n            recursive_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=self.max_chunk_size,\n                chunk_overlap=10,\n                separators=[\"\\n\\n\", \"\\n\", \".\", \", \", \" \", \"\"],\n            )\n            for bad_sentence in bad_sentences:\n                if len(bad_sentence) >= self.min_chunk_size:\n                    chunks.extend(recursive_splitter.split_text(bad_sentence))\n        return chunks\n\n\nclass MySentenceTransformer(SentenceTransformer):\n    def batch_encode(\n        self,\n        sentences: str | list[str],\n        prompt_name: str | None = None,\n        prompt: str | None = None,\n        batch_size: int = 32,\n        output_value: (\n            Literal[\"sentence_embedding\", \"token_embeddings\"] | None\n        ) = \"sentence_embedding\",\n        precision: Literal[\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"] = \"float32\",\n        convert_to_numpy: bool = True,\n        convert_to_tensor: bool = False,\n        device: str = None,\n        normalize_embeddings: bool = False,\n        **kwargs,\n    ) -> list[Tensor] | np.ndarray | Tensor:\n        if self.device.type == \"hpu\" and not self.is_hpu_graph_enabled:\n            import habana_frameworks.torch as ht\n\n            ht.hpu.wrap_in_hpu_graph(self, disable_tensor_cache=True)\n            self.is_hpu_graph_enabled = True\n\n        self.eval()\n        if convert_to_tensor:\n            convert_to_numpy = False\n\n        if output_value != \"sentence_embedding\":\n            convert_to_tensor = False\n            convert_to_numpy = False\n\n        input_was_string = False\n        if isinstance(sentences, str) or not hasattr(\n            sentences, \"__len__\"\n        ):  # Cast an individual sentence to a list with length 1\n            sentences = [sentences]\n            input_was_string = True\n\n        if prompt is None:\n            if prompt_name is not None:\n                try:\n                    prompt = self.prompts[prompt_name]\n                except KeyError:\n                    raise ValueError(\n                        f\"Prompt name '{prompt_name}' not found in the configured prompts dictionary with keys {list(self.prompts.keys())!r}.\"\n                    )\n            elif self.default_prompt_name is not None:\n                prompt = self.prompts.get(self.default_prompt_name, None)\n        else:\n            if prompt_name is not None:\n                warnings.warn(\n                    \"Encode with either a `prompt`, a `prompt_name`, or neither, but not both. \"\n                    \"Ignoring the `prompt_name` in favor of `prompt`.\"\n                )\n\n        extra_features = {}\n        if prompt is not None:\n            sentences = [prompt + sentence for sentence in sentences]\n\n            # Some models (e.g. INSTRUCTOR, GRIT) require removing the prompt before pooling\n            # Tracking the prompt length allow us to remove the prompt during pooling\n            tokenized_prompt = self.tokenize([prompt])\n            if \"input_ids\" in tokenized_prompt:\n                extra_features[\"prompt_length\"] = (\n                    tokenized_prompt[\"input_ids\"].shape[-1] - 1\n                )\n\n        if device is None:\n            device = self.device\n        else:\n            device = torch.device(device)\n\n        self.to(device)\n\n        all_embeddings = []\n        tokenized_sentences = self.tokenizer(sentences, verbose=False)[\"input_ids\"]\n        batchifyer = SimilarLengthsBatchifyer(batch_size, tokenized_sentences)\n        sentences = np.array(sentences)\n        batch_indices = []\n        for index_batch in batchifyer:\n            batch_indices.append(index_batch)\n            sentences_batch = sentences[index_batch]\n            features = self.tokenize(sentences_batch)\n            if self.device.type == \"hpu\":\n                if \"input_ids\" in features:\n                    curr_tokenize_len = features[\"input_ids\"].shape\n                    additional_pad_len = (\n                        2 ** math.ceil(math.log2(curr_tokenize_len[1]))\n                        - curr_tokenize_len[1]\n                    )\n                    features[\"input_ids\"] = torch.cat(\n                        (\n                            features[\"input_ids\"],\n                            torch.ones(\n                                (curr_tokenize_len[0], additional_pad_len),\n                                dtype=torch.int8,\n                            ),\n                        ),\n                        -1,\n                    )\n                    features[\"attention_mask\"] = torch.cat(\n                        (\n                            features[\"attention_mask\"],\n                            torch.zeros(\n                                (curr_tokenize_len[0], additional_pad_len),\n                                dtype=torch.int8,\n                            ),\n                        ),\n                        -1,\n                    )\n                    if \"token_type_ids\" in features:\n                        features[\"token_type_ids\"] = torch.cat(\n                            (\n                                features[\"token_type_ids\"],\n                                torch.zeros(\n                                    (curr_tokenize_len[0], additional_pad_len),\n                                    dtype=torch.int8,\n                                ),\n                            ),\n                            -1,\n                        )\n\n            features = batch_to_device(features, device)\n            features.update(extra_features)\n\n            with torch.no_grad():\n                out_features = self.forward(features, **kwargs)\n                if self.device.type == \"hpu\":\n                    out_features = copy.deepcopy(out_features)\n\n                out_features[\"sentence_embedding\"] = truncate_embeddings(\n                    out_features[\"sentence_embedding\"], self.truncate_dim\n                )\n\n                if output_value == \"token_embeddings\":\n                    embeddings = []\n                    for token_emb, attention in zip(\n                        out_features[output_value], out_features[\"attention_mask\"]\n                    ):\n                        last_mask_id = len(attention) - 1\n                        while last_mask_id > 0 and attention[last_mask_id].item() == 0:\n                            last_mask_id -= 1\n\n                        embeddings.append(token_emb[0 : last_mask_id + 1])\n                elif output_value is None:  # Return all outputs\n                    embeddings = []\n                    for sent_idx in range(len(out_features[\"sentence_embedding\"])):\n                        row = {\n                            name: out_features[name][sent_idx] for name in out_features\n                        }\n                        embeddings.append(row)\n                else:  # Sentence embeddings\n                    embeddings = out_features[output_value]\n                    embeddings = embeddings.detach()\n                    if normalize_embeddings:\n                        embeddings = torch.nn.functional.normalize(\n                            embeddings, p=2, dim=1\n                        )\n\n                    # fixes for #522 and #487 to avoid oom problems on gpu with large datasets\n                    if convert_to_numpy:\n                        embeddings = embeddings.to(\"cpu\", non_blocking=True)\n                        sync_device(device)\n\n                all_embeddings.extend(embeddings)\n\n        # Restore order after SimilarLengthsBatchifyer disrupted it:\n        # Ensure that the order of 'indices' and 'values' matches the order of the 'texts' parameter\n        batch_indices = np.concatenate(batch_indices)\n        sorted_indices = np.argsort(batch_indices)\n        all_embeddings = [all_embeddings[i] for i in sorted_indices]\n\n        if precision and precision != \"float32\":\n            all_embeddings = quantize_embeddings(all_embeddings, precision=precision)\n\n        if convert_to_tensor:\n            if len(all_embeddings):\n                if isinstance(all_embeddings, np.ndarray):\n                    all_embeddings = torch.from_numpy(all_embeddings)\n            else:\n                all_embeddings = torch.Tensor()\n        elif convert_to_numpy:\n            if not isinstance(all_embeddings, np.ndarray):\n                if all_embeddings and all_embeddings[0].dtype == torch.bfloat16:\n                    all_embeddings = np.asarray(\n                        [emb.float().numpy() for emb in all_embeddings]\n                    )\n                else:\n                    all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n        elif isinstance(all_embeddings, np.ndarray):\n            all_embeddings = [\n                torch.from_numpy(embedding) for embedding in all_embeddings\n            ]\n\n        if input_was_string:\n            all_embeddings = all_embeddings[0]\n\n        return all_embeddings\n\n\ndef sync_device(device: torch.device):\n    if device.type == \"cpu\":\n        return\n    elif device.type == \"cuda\":\n        torch.cuda.synchronize()\n    elif device.type == \"mps\":\n        torch.mps.synchronize()\n    elif device.type == \"xpu\":\n        torch.xpu.synchronize(device)\n    else:\n        warnings.warn(\n            \"Device type does not match 'cuda', 'xpu' or 'mps'. Not synchronizing\"\n        )\n\n\nclass DenseRetriever:\n\n    def __init__(\n        self,\n        embedding_model: MySentenceTransformer,\n        num_results: int = 5,\n        similarity_threshold: float = 0.5,\n    ):\n        self.embedding_model = embedding_model\n        self.num_results = num_results\n        self.similarity_threshold = similarity_threshold\n        self.knn = NearestNeighbors(n_neighbors=num_results)\n        self.documents = None\n        self.document_embeddings = None\n\n    def add_documents(self, documents: List[Document]):\n        self.documents = documents\n        self.document_embeddings = self.embedding_model.batch_encode(\n            [doc.page_content for doc in documents]\n        )\n        self.knn.fit(self.document_embeddings)\n\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        query_embedding = self.embedding_model.encode(query)\n\n        _, neighbor_indices = self.knn.kneighbors(query_embedding.reshape(1, -1))\n        neighbor_indices = neighbor_indices.squeeze(0)\n\n        relevant_doc_embeddings = self.document_embeddings[neighbor_indices]\n        # Filter out redundant documents\n        included_idxs = filter_similar_embeddings(\n            relevant_doc_embeddings, cosine_similarity, threshold=0.95\n        )\n        relevant_doc_embeddings = relevant_doc_embeddings[included_idxs]\n\n        # Filter out documents that aren't similar enough\n        similarity = cosine_similarity([query_embedding], relevant_doc_embeddings)[0]\n        similar_enough = np.where(similarity > self.similarity_threshold)[0]\n        included_idxs = [included_idxs[i] for i in similar_enough]\n\n        filtered_result_indices = neighbor_indices[included_idxs]\n        return [self.documents[i] for i in filtered_result_indices]\n\n\ndef filter_similar_embeddings(\n    embedded_documents: List[List[float]], similarity_fn: Callable, threshold: float\n) -> List[int]:\n    \"\"\"Filter redundant documents based on the similarity of their embeddings.\"\"\"\n    similarity = np.tril(similarity_fn(embedded_documents, embedded_documents), k=-1)\n    redundant = np.where(similarity > threshold)\n    redundant_stacked = np.column_stack(redundant)\n    redundant_sorted = np.argsort(similarity[redundant])[::-1]\n    included_idxs = set(range(len(embedded_documents)))\n    for first_idx, second_idx in redundant_stacked[redundant_sorted]:\n        if first_idx in included_idxs and second_idx in included_idxs:\n            # Default to dropping the second document of any highly similar pair.\n            included_idxs.remove(second_idx)\n    return list(sorted(included_idxs))\n\n\nclass SimilarLengthsBatchifyer:\n    \"\"\"\n    Generator class to split samples into batches. Groups sample sequences\n    of equal/similar length together to minimize the need for padding within a batch.\n    \"\"\"\n\n    def __init__(self, batch_size, inputs, max_padding_len=10):\n        # Remember number of samples\n        self.num_samples = len(inputs)\n\n        self.unique_lengths = set()\n        self.length_to_sample_indices = {}\n\n        for i in range(0, len(inputs)):\n            len_input = len(inputs[i])\n\n            self.unique_lengths.add(len_input)\n\n            # For each length, keep track of the indices of the samples that have this length\n            # E.g.: self.length_to_sample_indices = { 3: [3,5,11], 4: [1,2], ...}\n            if len_input in self.length_to_sample_indices:\n                self.length_to_sample_indices[len_input].append(i)\n            else:\n                self.length_to_sample_indices[len_input] = [i]\n\n        # Use a dynamic batch size to speed up inference at a constant VRAM usage\n        self.unique_lengths = sorted(list(self.unique_lengths))\n        max_chars_per_batch = self.unique_lengths[-1] * batch_size\n        self.length_to_batch_size = {\n            length: int(max_chars_per_batch / (length * batch_size)) * batch_size\n            for length in self.unique_lengths\n        }\n\n        # Merge samples of similar lengths in those cases where the amount of samples\n        # of a particular length is < dynamic batch size\n        accum_len_diff = 0\n        for i in range(1, len(self.unique_lengths)):\n            if accum_len_diff >= max_padding_len:\n                accum_len_diff = 0\n                continue\n            curr_len = self.unique_lengths[i]\n            prev_len = self.unique_lengths[i - 1]\n            len_diff = curr_len - prev_len\n            if len_diff <= max_padding_len and (\n                len(self.length_to_sample_indices[curr_len])\n                < self.length_to_batch_size[curr_len]\n                or len(self.length_to_sample_indices[prev_len])\n                < self.length_to_batch_size[prev_len]\n            ):\n                self.length_to_sample_indices[curr_len].extend(\n                    self.length_to_sample_indices[prev_len]\n                )\n                self.length_to_sample_indices[prev_len] = []\n                accum_len_diff += len_diff\n            else:\n                accum_len_diff = 0\n\n    def __len__(self):\n        return self.num_samples\n\n    def __iter__(self):\n        # Iterate over all possible sentence lengths\n        for length in self.unique_lengths:\n\n            # Get indices of all samples for the current length\n            # for example, all indices of samples with a length of 7\n            sequence_indices = self.length_to_sample_indices[length]\n            if len(sequence_indices) == 0:\n                continue\n\n            dyn_batch_size = self.length_to_batch_size[length]\n\n            # Compute the number of batches\n            num_batches = np.ceil(len(sequence_indices) / dyn_batch_size)\n\n            # Loop over all possible batches\n            for batch_indices in np.array_split(sequence_indices, num_batches):\n                yield batch_indices\n\n\ndef neg_dot_dist(x, y):\n    dist = np.dot(x, y).data\n    if dist.size == 0:  # no overlapping non-zero entries between x and y\n        return np.inf\n    return -dist.sum()\n\n\nclass SpladeRetriever:\n    def __init__(\n        self,\n        splade_doc_tokenizer,\n        splade_doc_model,\n        splade_query_tokenizer,\n        splade_query_model,\n        device,\n        batch_size,\n        k,\n    ):\n        self.splade_doc_tokenizer = splade_doc_tokenizer\n        self.splade_doc_model = splade_doc_model\n        self.splade_query_tokenizer = splade_query_tokenizer\n        self.splade_query_model = splade_query_model\n        self.device = device\n        self.batch_size = batch_size\n        self.k = k\n        self.vocab_size = splade_doc_model.config.vocab_size\n        self.texts: List[str] = []\n        self.metadatas: List[Dict] = []\n        self.sparse_doc_vecs: List[csr_array] = []\n\n    def compute_document_vectors(\n        self, texts: List[str], batch_size: int\n    ) -> Tuple[List[List[int]], List[List[float]]]:\n        indices = []\n        values = []\n        tokenized_texts = self.splade_doc_tokenizer(\n            texts, truncation=False, padding=False, return_tensors=\"np\"\n        )[\"input_ids\"]\n        batchifyer = SimilarLengthsBatchifyer(batch_size, tokenized_texts)\n        texts = np.array(texts)\n        batch_indices = []\n        for index_batch in batchifyer:\n            batch_indices.append(index_batch)\n            with torch.no_grad():\n                tokens = self.splade_doc_tokenizer(\n                    texts[index_batch].tolist(),\n                    truncation=True,\n                    padding=True,\n                    return_tensors=\"pt\",\n                ).to(self.device)\n                output = self.splade_doc_model(**tokens)\n            logits, attention_mask = output.logits, tokens.attention_mask\n            relu_log = torch.log(1 + torch.relu(logits))\n            weighted_log = relu_log * attention_mask.unsqueeze(-1)\n            tvecs, _ = torch.max(weighted_log, dim=1)\n\n            # extract all non-zero values and their indices from the sparse vectors\n            for batch in tvecs.cpu().to(torch.float32):\n                indices.append(batch.nonzero(as_tuple=True)[0].numpy())\n                values.append(batch[indices[-1]].numpy())\n\n        # Restore order after SimilarLengthsBatchifyer disrupted it:\n        # Ensure that the order of 'indices' and 'values' matches the order of the 'texts' parameter\n        batch_indices = np.concatenate(batch_indices)\n        sorted_indices = np.argsort(batch_indices)\n        indices = [indices[i] for i in sorted_indices]\n        values = [values[i] for i in sorted_indices]\n        return indices, values\n\n    def compute_query_vector(self, text: str):\n        \"\"\"\n        Computes a vector from logits and attention mask using ReLU, log, and max operations.\n        \"\"\"\n        with torch.no_grad():\n            tokens = self.splade_query_tokenizer(text, return_tensors=\"pt\").to(\n                self.device\n            )\n            output = self.splade_query_model(**tokens)\n        logits, attention_mask = output.logits, tokens.attention_mask\n        relu_log = torch.log(1 + torch.relu(logits))\n        weighted_log = relu_log * attention_mask.unsqueeze(-1)\n        max_val, _ = torch.max(weighted_log, dim=1)\n        query_vec = max_val.squeeze().cpu().to(torch.float32)\n\n        query_indices = query_vec.nonzero().numpy().flatten()\n        query_values = query_vec.detach().numpy()[query_indices]\n\n        return query_indices, query_values\n\n    def add_documents(self, documents: List[Document]) -> List[str]:\n        \"\"\"Run more documents through the embeddings and add to the vectorstore.\n\n        Args:\n            documents (List[Document]: Documents to add to the vectorstore.\n\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        texts = [doc.page_content for doc in documents]\n        metadatas = [doc.metadata for doc in documents]\n        return self.add_texts(texts, metadatas)\n\n    def add_texts(self, texts: Iterable[str], metadatas: Optional[List[dict]] = None):\n\n        # Remove duplicate and empty texts\n        text_to_metadata = {\n            texts[i]: metadatas[i] for i in range(len(texts)) if len(texts[i]) > 0\n        }\n        texts = list(text_to_metadata.keys())\n        metadatas = list(text_to_metadata.values())\n        self.texts = texts\n        self.metadatas = metadatas\n\n        indices, values = self.compute_document_vectors(texts, self.batch_size)\n        self.sparse_doc_vecs = [\n            csr_array((val, (ind,)), shape=(self.vocab_size,))\n            for val, ind in zip(values, indices)\n        ]\n\n        if self.device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        query_indices, query_values = self.compute_query_vector(query)\n\n        sparse_query_vec = csr_array(\n            (query_values, (query_indices,)), shape=(self.vocab_size,)\n        )\n        dists = [\n            neg_dot_dist(sparse_query_vec, doc_vec) for doc_vec in self.sparse_doc_vecs\n        ]\n        sorted_indices = np.argsort(dists)\n\n        return [\n            Document(self.texts[i], self.metadatas[i]) for i in sorted_indices[: self.k]\n        ]\n\n\ndef default_preprocessing_func(text: str) -> List[str]:\n    return text.split()\n\n\nclass BM25Retriever:\n    \"\"\"Adapted from Langchain:\n    https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/retrievers/bm25.py\n    \"\"\"\n\n    vectorizer: Any\n    \"\"\" BM25 vectorizer.\"\"\"\n    docs: List[Document]\n    \"\"\" List of documents.\"\"\"\n    k: int = 4\n    \"\"\" Number of documents to return.\"\"\"\n    preprocess_func: Callable[[str], List[str]] = default_preprocessing_func\n    \"\"\" Preprocessing function to use on the text before BM25 vectorization.\"\"\"\n\n    def __init__(\n        self,\n        vectorizer: Any,\n        docs: List[Document],\n        k: int = 4,\n        preprocess_func: Callable[[str], List[str]] = default_preprocessing_func,\n    ):\n        self.vectorizer = vectorizer\n        self.docs = docs\n        self.k = k\n        self.preprocess_func = preprocess_func\n\n    @classmethod\n    def from_texts(\n        cls,\n        texts: Iterable[str],\n        metadatas: Optional[Iterable[dict]] = None,\n        bm25_params: Optional[Dict[str, Any]] = None,\n        preprocess_func: Callable[[str], List[str]] = default_preprocessing_func,\n        **kwargs: Any,\n    ) -> \"BM25Retriever\":\n        \"\"\"\n        Create a BM25Retriever from a list of texts.\n        Args:\n            texts: A list of texts to vectorize.\n            metadatas: A list of metadata dicts to associate with each text.\n            bm25_params: Parameters to pass to the BM25 vectorizer.\n            preprocess_func: A function to preprocess each text before vectorization.\n            **kwargs: Any other arguments to pass to the retriever.\n\n        Returns:\n            A BM25Retriever instance.\n        \"\"\"\n        texts_processed = [preprocess_func(t) for t in texts]\n        bm25_params = bm25_params or {}\n        vectorizer = BM25Okapi(texts_processed, **bm25_params)\n        metadatas = metadatas or ({} for _ in texts)\n        docs = [Document(page_content=t, metadata=m) for t, m in zip(texts, metadatas)]\n        return cls(\n            vectorizer=vectorizer, docs=docs, preprocess_func=preprocess_func, **kwargs\n        )\n\n    @classmethod\n    def from_documents(\n        cls,\n        documents: Iterable[Document],\n        *,\n        bm25_params: Optional[Dict[str, Any]] = None,\n        preprocess_func: Callable[[str], List[str]] = default_preprocessing_func,\n        **kwargs: Any,\n    ) -> \"BM25Retriever\":\n        \"\"\"\n        Create a BM25Retriever from a list of Documents.\n        Args:\n            documents: A list of Documents to vectorize.\n            bm25_params: Parameters to pass to the BM25 vectorizer.\n            preprocess_func: A function to preprocess each text before vectorization.\n            **kwargs: Any other arguments to pass to the retriever.\n\n        Returns:\n            A BM25Retriever instance.\n        \"\"\"\n        texts, metadatas = zip(*((d.page_content, d.metadata) for d in documents))\n        return cls.from_texts(\n            texts=texts,\n            bm25_params=bm25_params,\n            metadatas=metadatas,\n            preprocess_func=preprocess_func,\n            **kwargs,\n        )\n\n    def get_relevant_documents(self, query: str) -> List[Document]:\n        processed_query = self.preprocess_func(query)\n        return_docs = self.vectorizer.get_top_n(processed_query, self.docs, n=self.k)\n        return return_docs\n\n\nasync def async_download_html(\n    url: str,\n    headers: Dict,\n    timeout: int,\n    proxy: str = None,\n    proxy_except_domains: tuple[str] = None,\n):\n    if proxy_except_domains and urlparse(url).netloc.endswith(proxy_except_domains):\n        proxy = None\n    async with aiohttp.ClientSession(\n        headers=headers,\n        timeout=aiohttp.ClientTimeout(timeout),\n        max_field_size=65536,\n        proxy=proxy,\n    ) as session:\n        try:\n            resp = await session.get(url)\n            return await resp.text(), url\n        except UnicodeDecodeError:\n            if not resp.headers[\"Content-Type\"].startswith(\"text/html\"):\n                print(\n                    f\"LLM_Web_search | {url} generated an exception: Expected content type text/html. Got {resp.headers['Content-Type']}.\"\n                )\n        except TimeoutError:\n            print(\"LLM_Web_search | %r did not load in time\" % url)\n        except Exception as exc:\n            print(\"LLM_Web_search | %r generated an exception: %s\" % (url, exc))\n    return None\n\n\nasync def async_fetch_chunk_websites(\n    urls: List[str],\n    text_splitter: BoundedSemanticChunker or RecursiveCharacterTextSplitter,\n    timeout: int = 10,\n    proxy: str = None,\n    proxy_except_domains: tuple[str] = None,\n):\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n        \"Accept-Language\": \"en-US,en;q=0.5\",\n        \"Accept-Encoding\": \"gzip;q=1, *;q=0.5\",\n    }\n    result_futures = [\n        async_download_html(url, headers, timeout, proxy, proxy_except_domains)\n        for url in urls\n    ]\n    chunks = []\n    loop = asyncio.get_running_loop()\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        for f in asyncio.as_completed(result_futures):\n            result = await f\n            if result:\n                resp_html, url = result\n                document = html_to_plaintext_doc(resp_html, url)\n                new_chunks = await loop.run_in_executor(\n                    pool, text_splitter.split_documents, [document]\n                )\n                chunks.extend(new_chunks)\n    return chunks\n\n\ndef docs_to_pretty_str(docs) -> str:\n    ret_str = \"\"\n    for i, doc in enumerate(docs):\n        ret_str += f\"Result {i + 1}:\\n\"\n        ret_str += f\"{doc.page_content}\\n\"\n        ret_str += f\"Source URL: {doc.metadata['source']}\\n\"\n    return ret_str\n\n\ndef html_to_plaintext_doc(html_text: str or bytes, url: str) -> Document:\n    with warnings.catch_warnings(action=\"ignore\"):\n        soup = BeautifulSoup(html_text, features=\"lxml\")\n    for script in soup([\"script\", \"style\"]):\n        script.extract()\n\n    strings = \"\\n\".join([s.strip() for s in soup.stripped_strings])\n    webpage_document = Document(page_content=strings, metadata={\"source\": url})\n    return webpage_document\n\n\ndef weighted_reciprocal_rank(\n    doc_lists: List[List[Document]], weights: List[float], c: int = 60\n) -> List[Document]:\n    \"\"\"\n    Perform weighted Reciprocal Rank Fusion on multiple rank lists.\n    You can find more details about RRF here:\n    https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n\n    Args:\n        doc_lists: A list of rank lists, where each rank list contains unique items.\n        weights: A list of weights corresponding to the rank lists. Defaults to equal\n            weighting for all lists.\n        c: A constant added to the rank, controlling the balance between the importance\n            of high-ranked items and the consideration given to lower-ranked items.\n            Default is 60.\n\n    Returns:\n        list: The final aggregated list of items sorted by their weighted RRF\n                scores in descending order.\n    \"\"\"\n    if len(doc_lists) != len(weights):\n        raise ValueError(\"Number of rank lists must be equal to the number of weights.\")\n\n    # Associate each doc's content with its RRF score for later sorting by it\n    # Duplicated contents across retrievers are collapsed & scored cumulatively\n    rrf_score: Dict[str, float] = defaultdict(float)\n    for doc_list, weight in zip(doc_lists, weights):\n        for rank, doc in enumerate(doc_list, start=1):\n            rrf_score[doc.page_content] += weight / (rank + c)\n\n    # Docs are deduplicated by their contents then sorted by their scores\n    all_docs = chain.from_iterable(doc_lists)\n    sorted_docs = sorted(\n        unique_by_key(all_docs, lambda doc: doc.page_content),\n        reverse=True,\n        key=lambda doc: rrf_score[doc.page_content],\n    )\n    return sorted_docs\n\n\ndef unique_by_key(iterable: Iterable, key: Callable) -> Iterator:\n    \"\"\"Yield unique elements of an iterable based on a key function.\n\n    Args:\n        iterable: The iterable to filter.\n        key: A function that returns a hashable key for each element.\n\n    Yields:\n        Unique elements of the iterable based on the key function.\n    \"\"\"\n    seen = set()\n    for e in iterable:\n        if (k := key(e)) not in seen:\n            seen.add(k)\n            yield e\n","specs":[{"name":"no_tool_necessary","description":"Choose this tool if you can answer the user without using any tool.","parameters":{"properties":{},"type":"object"}},{"name":"reuse_existing_web_search_results","description":"Choose this tool if existing search results from a previous web search can be used to answer the user's query.","parameters":{"properties":{},"type":"object"}},{"name":"search_web","description":"The search tool will search the web and return the results. You must formulate your own search query based on the user's message.","parameters":{"properties":{"query":{"type":"string"}},"required":["query"],"type":"object"}}],"meta":{"description":"Enables LLMs to search the web. Now with proxy support! Github: https://github.com/mamei16/LLM_Web_search_OWUI","manifest":{"version":"0.4.0"}},"access_control":{},"updated_at":1754194257,"created_at":1751333630},{"id":"image_generation_using_pollination_ai","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"Image generation using pollination ai","content":"import requests\nimport base64\nfrom typing import Dict, Any, Optional\nfrom pydantic import BaseModel, Field\nimport logging\nfrom requests.exceptions import Timeout, RequestException\nfrom urllib.parse import quote\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass Tools:\n    class Valves(BaseModel):\n        BASE_URL: str = Field(\n            default=\"https://image.pollinations.ai/prompt/\",\n            description=\"Pollinations API URL for text to image generation\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n\n    async def create_image(\n        self,\n        prompt: str,\n        image_format: str = \"default\",\n        model: Optional[str] = \"flux-realism\",\n        seed: Optional[int] = None,\n        nologo: bool = True,\n        private: bool = True,\n        enhance: bool = True,\n        __user__: dict = {},\n        __event_emitter__=None,\n    ) -> str:\n        \"\"\"\n        Creates visually stunning images using the Pollinations.ai API.\n\n        Args:\n            prompt: The text prompt to generate the image from\n            image_format: Format of the image (default, landscape, portrait, etc.)\n            model: The model to use for generation (optional)\n            seed: Random seed for reproducible results (optional)\n            nologo: Whether to remove the Pollinations logo\n            private: Whether to make the generation private\n            enhance: Whether to enhance the image quality\n\n        Returns:\n            str: Message indicating success or failure\n        \"\"\"\n        logger.debug(\"Starting create_image function\")\n\n        try:\n            # Define supported image formats\n            formats = {\n                \"default\": (1024, 1024),\n                \"square\": (1024, 1024),\n                \"landscape\": (1024, 768),\n                \"landscape_large\": (1440, 1024),\n                \"portrait\": (768, 1024),\n                \"portrait_large\": (1024, 1440),\n            }\n\n            logger.debug(f\"Validating format: {image_format}\")\n            if image_format not in formats:\n                raise ValueError(\n                    f\"Invalid format. Must be one of: {', '.join(formats.keys())}\"\n                )\n\n            width, height = formats[image_format]\n            logger.debug(f\"Using dimensions: {width}x{height}\")\n\n            # Notify start of generation\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\"description\": \"Generating image\", \"done\": False},\n                }\n            )\n\n            # Build the URL with parameters\n            encoded_prompt = quote(prompt)\n            url = f\"{self.valves.BASE_URL}{encoded_prompt}\"\n\n            params = {\"width\": width, \"height\": height}\n\n            if model:\n                params[\"model\"] = model\n            if seed is not None:\n                params[\"seed\"] = seed\n            if nologo:\n                params[\"nologo\"] = \"true\"\n            if private:\n                params[\"private\"] = \"true\"\n            if enhance:\n                params[\"enhance\"] = \"true\"\n\n            # Make the GET request\n            response = requests.get(url, params=params, timeout=(10, 600))\n\n            if response.status_code != 200:\n                logger.error(f\"API request failed: {response.text}\")\n                raise RequestException(\n                    f\"API request failed with status code {response.status_code}\"\n                )\n\n            # Convert image to base64\n            image_content = response.content\n            image_url = f\"data:image/jpeg;base64,{base64.b64encode(image_content).decode('utf-8')}\"\n\n            # Send completion status\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\"description\": \"Image generated\", \"done\": True},\n                }\n            )\n\n            # Send the image\n            await __event_emitter__(\n                {\n                    \"type\": \"message\",\n                    \"data\": {\n                        \"content\": f\"Generated image for prompt: '{prompt}'\\n\\n![Generated Image]({image_url})\"\n                    },\n                }\n            )\n\n            return f\"Image successfully generated for prompt: '{prompt}'\"\n\n        except Timeout:\n            error_msg = (\n                \"Request timed out while generating the image. Please try again later.\"\n            )\n            logger.error(error_msg)\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\"description\": error_msg, \"done\": True},\n                }\n            )\n            return error_msg\n\n        except RequestException as e:\n            error_msg = f\"Network error occurred: {str(e)}\"\n            logger.error(error_msg)\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\"description\": error_msg, \"done\": True},\n                }\n            )\n            return error_msg\n\n        except Exception as e:\n            error_msg = f\"An error occurred: {str(e)}\"\n            logger.error(f\"Unexpected error: {str(e)}\")\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\"description\": error_msg, \"done\": True},\n                }\n            )\n            return error_msg\n","specs":[{"name":"create_image","description":"Creates visually stunning images using the Pollinations.ai API.\n\nArgs:\nprompt: The text prompt to generate the image from\nimage_format: Format of the image (default, landscape, portrait, etc.)\nmodel: The model to use for generation (optional)\nseed: Random seed for reproducible results (optional)\nnologo: Whether to remove the Pollinations logo\nprivate: Whether to make the generation private\nenhance: Whether to enhance the image quality\n\nReturns:\nstr: Message indicating success or failure","parameters":{"properties":{"prompt":{"type":"string"},"image_format":{"default":"default","type":"string"},"model":{"anyOf":[{"type":"string"},{"type":"null"}],"default":"flux-realism"},"seed":{"anyOf":[{"type":"integer"},{"type":"null"}],"default":null},"nologo":{"default":true,"type":"boolean"},"private":{"default":true,"type":"boolean"},"enhance":{"default":true,"type":"boolean"}},"required":["prompt"],"type":"object"}}],"meta":{"description":"Free image generation tool no api needed","manifest":{}},"access_control":{},"updated_at":1754183110,"created_at":1751333813},{"id":"todo","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"todo","content":"import os\nimport requests\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\n\n\nclass Tools:\n    def __init__(self):\n        pass\n\n    # Add your custom tools using pure Python code here, make sure to add type hints and descriptions\n\n    def get_user_name_and_email_and_id(self, __user__: dict = {}) -> str:\n        \"\"\"\n        Get the user name, Email and ID from the user object.\n        \"\"\"\n\n        # Do not include a descrption for __user__ as it should not be shown in the tool's specification\n        # The session user object will be passed as a parameter when the function is called\n\n        print(__user__)\n        result = \"\"\n\n        if \"name\" in __user__:\n            result += f\"User: {__user__['name']}\"\n        if \"id\" in __user__:\n            result += f\" (ID: {__user__['id']})\"\n        if \"email\" in __user__:\n            result += f\" (Email: {__user__['email']})\"\n\n        if result == \"\":\n            result = \"User: Unknown\"\n\n        return result\n\n    def get_current_time(self) -> str:\n        \"\"\"\n        Get the current time in a more human-readable format.\n        \"\"\"\n\n        now = datetime.now()\n        current_time = now.strftime(\"%I:%M:%S %p\")  # Using 12-hour format with AM/PM\n        current_date = now.strftime(\n            \"%A, %B %d, %Y\"\n        )  # Full weekday, month name, day, and year\n\n        return f\"Current Date and Time = {current_date}, {current_time}\"\n\n    def calculator(\n        self,\n        equation: str = Field(\n            ..., description=\"The mathematical equation to calculate.\"\n        ),\n    ) -> str:\n        \"\"\"\n        Calculate the result of an equation.\n        \"\"\"\n\n        # Avoid using eval in production code\n        # https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html\n        try:\n            result = eval(equation)\n            return f\"{equation} = {result}\"\n        except Exception as e:\n            print(e)\n            return \"Invalid equation\"\n\n    def get_current_weather(\n        self,\n        city: str = Field(\n            \"New York, NY\", description=\"Get the current weather for a given city.\"\n        ),\n    ) -> str:\n        \"\"\"\n        Get the current weather for a given city.\n        \"\"\"\n\n        api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n        if not api_key:\n            return (\n                \"API key is not set in the environment variable 'OPENWEATHER_API_KEY'.\"\n            )\n\n        base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n        params = {\n            \"q\": city,\n            \"appid\": api_key,\n            \"units\": \"metric\",  # Optional: Use 'imperial' for Fahrenheit\n        }\n\n        try:\n            response = requests.get(base_url, params=params)\n            response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n            data = response.json()\n\n            if data.get(\"cod\") != 200:\n                return f\"Error fetching weather data: {data.get('message')}\"\n\n            weather_description = data[\"weather\"][0][\"description\"]\n            temperature = data[\"main\"][\"temp\"]\n            humidity = data[\"main\"][\"humidity\"]\n            wind_speed = data[\"wind\"][\"speed\"]\n\n            return f\"Weather in {city}: {temperature}°C\"\n        except requests.RequestException as e:\n            return f\"Error fetching weather data: {str(e)}\"\n","specs":[{"name":"calculator","description":"Calculate the result of an equation.","parameters":{"properties":{"equation":{"description":"The mathematical equation to calculate.","type":"string"}},"required":["equation"],"type":"object"}},{"name":"get_current_time","description":"Get the current time in a more human-readable format.","parameters":{"properties":{},"type":"object"}},{"name":"get_current_weather","description":"Get the current weather for a given city.","parameters":{"properties":{"city":{"default":"New York, NY","description":"Get the current weather for a given city.","type":"string"}},"type":"object"}},{"name":"get_user_name_and_email_and_id","description":"Get the user name, Email and ID from the user object.","parameters":{"properties":{},"type":"object"}}],"meta":{"description":"todo","manifest":{}},"access_control":{},"updated_at":1754182285,"created_at":1754182285},{"id":"time","user_id":"c2f1fa7b-360f-4003-9488-76d21a07c5ae","name":"time","content":"import os\nimport requests\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\n\n\nclass Tools:\n    def __init__(self):\n        pass\n\n    # Add your custom tools using pure Python code here, make sure to add type hints and descriptions\n\n    def get_user_name_and_email_and_id(self, __user__: dict = {}) -> str:\n        \"\"\"\n        Get the user name, Email and ID from the user object.\n        \"\"\"\n\n        # Do not include a descrption for __user__ as it should not be shown in the tool's specification\n        # The session user object will be passed as a parameter when the function is called\n\n        print(__user__)\n        result = \"\"\n\n        if \"name\" in __user__:\n            result += f\"User: {__user__['name']}\"\n        if \"id\" in __user__:\n            result += f\" (ID: {__user__['id']})\"\n        if \"email\" in __user__:\n            result += f\" (Email: {__user__['email']})\"\n\n        if result == \"\":\n            result = \"User: Unknown\"\n\n        return result\n\n    def get_current_time(self) -> str:\n        \"\"\"\n        Get the current time in a more human-readable format.\n        \"\"\"\n\n        now = datetime.now()\n        current_time = now.strftime(\"%I:%M:%S %p\")  # Using 12-hour format with AM/PM\n        current_date = now.strftime(\n            \"%A, %B %d, %Y\"\n        )  # Full weekday, month name, day, and year\n\n        return f\"Current Date and Time = {current_date}, {current_time}\"\n\n    def calculator(\n        self,\n        equation: str = Field(\n            ..., description=\"The mathematical equation to calculate.\"\n        ),\n    ) -> str:\n        \"\"\"\n        Calculate the result of an equation.\n        \"\"\"\n\n        # Avoid using eval in production code\n        # https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html\n        try:\n            result = eval(equation)\n            return f\"{equation} = {result}\"\n        except Exception as e:\n            print(e)\n            return \"Invalid equation\"\n\n    def get_current_weather(\n        self,\n        city: str = Field(\n            \"New York, NY\", description=\"Get the current weather for a given city.\"\n        ),\n    ) -> str:\n        \"\"\"\n        Get the current weather for a given city.\n        \"\"\"\n\n        api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n        if not api_key:\n            return (\n                \"API key is not set in the environment variable 'OPENWEATHER_API_KEY'.\"\n            )\n\n        base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n        params = {\n            \"q\": city,\n            \"appid\": api_key,\n            \"units\": \"metric\",  # Optional: Use 'imperial' for Fahrenheit\n        }\n\n        try:\n            response = requests.get(base_url, params=params)\n            response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n            data = response.json()\n\n            if data.get(\"cod\") != 200:\n                return f\"Error fetching weather data: {data.get('message')}\"\n\n            weather_description = data[\"weather\"][0][\"description\"]\n            temperature = data[\"main\"][\"temp\"]\n            humidity = data[\"main\"][\"humidity\"]\n            wind_speed = data[\"wind\"][\"speed\"]\n\n            return f\"Weather in {city}: {temperature}°C\"\n        except requests.RequestException as e:\n            return f\"Error fetching weather data: {str(e)}\"\n","specs":[{"name":"calculator","description":"Calculate the result of an equation.","parameters":{"properties":{"equation":{"description":"The mathematical equation to calculate.","type":"string"}},"required":["equation"],"type":"object"}},{"name":"get_current_time","description":"Get the current time in a more human-readable format.","parameters":{"properties":{},"type":"object"}},{"name":"get_current_weather","description":"Get the current weather for a given city.","parameters":{"properties":{"city":{"default":"New York, NY","description":"Get the current weather for a given city.","type":"string"}},"type":"object"}},{"name":"get_user_name_and_email_and_id","description":"Get the user name, Email and ID from the user object.","parameters":{"properties":{},"type":"object"}}],"meta":{"description":"time","manifest":{}},"access_control":{},"updated_at":1753328140,"created_at":1751215124},{"id":"enhanced_web_scrape","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"Enhanced Web Scrape","content":"\"\"\"\ntitle: Enhanced Web Scrape\nauthor: ekatiyar\nauthor_url: https://github.com/ekatiyar\ngit_url: https://github.com/ekatiyar/open-webui-tools\ndescription: An improved web scraping tool that extracts text content using Jina Reader, now with better filtering, user-configuration, and UI feedback using emitters.\noriginal_author: Pyotr Growpotkin\noriginal_author_url: https://github.com/christ-offer/\noriginal_git_url: https://github.com/christ-offer/open-webui-tools\nfunding_url: https://github.com/open-webui\nversion: 0.0.5\nlicense: MIT\n\"\"\"\n\nimport requests\nfrom typing import Callable, Any\nimport re\nfrom pydantic import BaseModel, Field\n\nimport unittest\n\n\ndef extract_title(text):\n    \"\"\"\n    Extracts the title from a string containing structured text.\n\n    :param text: The input string containing the title.\n    :return: The extracted title string, or None if the title is not found.\n    \"\"\"\n    match = re.search(r\"Title: (.*)\\n\", text)\n    return match.group(1).strip() if match else None\n\n\ndef clean_urls(text) -> str:\n    \"\"\"\n    Cleans URLs from a string containing structured text.\n\n    :param text: The input string containing the URLs.\n    :return: The cleaned string with URLs removed.\n    \"\"\"\n    return re.sub(r\"\\((http[^)]+)\\)\", \"\", text)\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def progress_update(self, description):\n        await self.emit(description)\n\n    async def error_update(self, description):\n        await self.emit(description, \"error\", True)\n\n    async def success_update(self, description):\n        await self.emit(description, \"success\", True)\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        DISABLE_CACHING: bool = Field(\n            default=False, description=\"Bypass Jina Cache when scraping\"\n        )\n        GLOBAL_JINA_API_KEY: str = Field(\n            default=\"\",\n            description=\"(Optional) Jina API key. Allows a higher rate limit when scraping. Used when a User-specific API key is not available.\",\n        )\n        CITITATION: bool = Field(\n            default=\"True\", description=\"True or false for citation\"\n        )\n\n    class UserValves(BaseModel):\n        CLEAN_CONTENT: bool = Field(\n            default=True,\n            description=\"Remove links and image urls from scraped content. This reduces the number of tokens.\",\n        )\n        JINA_API_KEY: str = Field(\n            default=\"\",\n            description=\"(Optional) Jina API key. Allows a higher rate limit when scraping.\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.citation = self.valves.CITITATION\n\n    async def web_scrape(\n        self,\n        url: str,\n        __event_emitter__: Callable[[dict], Any] = None,\n        __user__: dict = {},\n    ) -> str:\n        \"\"\"\n        Scrape and process a web page using r.jina.ai\n\n        :param url: The URL of the web page to scrape.\n        :return: The scraped and processed webpage content, or an error message.\n        \"\"\"\n        emitter = EventEmitter(__event_emitter__)\n        if \"valves\" not in __user__:\n            __user__[\"valves\"] = self.UserValves()\n\n        await emitter.progress_update(f\"Scraping {url}\")\n        jina_url = f\"https://r.jina.ai/{url}\"\n\n        headers = {\n            \"X-No-Cache\": \"true\" if self.valves.DISABLE_CACHING else \"false\",\n            \"X-With-Generated-Alt\": \"true\",\n        }\n\n        if __user__[\"valves\"].JINA_API_KEY:\n            headers[\"Authorization\"] = f\"Bearer {__user__['valves'].JINA_API_KEY}\"\n        elif self.valves.GLOBAL_JINA_API_KEY:\n            headers[\"Authorization\"] = f\"Bearer {self.valves.GLOBAL_JINA_API_KEY}\"\n\n        try:\n            response = requests.get(jina_url, headers=headers)\n            response.raise_for_status()\n\n            should_clean = __user__[\"valves\"].CLEAN_CONTENT\n            if should_clean:\n                await emitter.progress_update(\"Received content, cleaning up ...\")\n            content = clean_urls(response.text) if should_clean else response.text\n\n            title = extract_title(content)\n            await emitter.success_update(\n                f\"Successfully Scraped {title if title else url}\"\n            )\n            return content\n\n        except requests.RequestException as e:\n            error_message = f\"Error scraping web page: {str(e)}\"\n            await emitter.error_update(error_message)\n            return error_message\n\n\nclass WebScrapeTest(unittest.IsolatedAsyncioTestCase):\n    async def test_web_scrape(self):\n        url = \"https://toscrape.com/\"\n        content = await Tools().web_scrape(url)\n        self.assertEqual(\"Scraping Sandbox\", extract_title(content))\n        self.assertEqual(len(content), 770)\n\n\nif __name__ == \"__main__\":\n    print(\"Running tests...\")\n    unittest.main()\n","specs":[{"name":"web_scrape","description":"Scrape and process a web page using r.jina.ai","parameters":{"properties":{"url":{"description":"The URL of the web page to scrape.","type":"string"}},"required":["url"],"type":"object"}}],"meta":{"description":"An improved web scraping tool that extracts text content using Jina Reader, now with better filtering, user-configuration, and UI feedback using emitters.","manifest":{"title":"Enhanced Web Scrape","author":"ekatiyar","author_url":"https://github.com/ekatiyar","git_url":"https://github.com/ekatiyar/open-webui-tools","description":"An improved web scraping tool that extracts text content using Jina Reader, now with better filtering, user-configuration, and UI feedback using emitters.","original_author":"Pyotr Growpotkin","original_author_url":"https://github.com/christ-offer/","original_git_url":"https://github.com/christ-offer/open-webui-tools","funding_url":"https://github.com/open-webui","version":"0.0.5","license":"MIT"}},"access_control":{},"updated_at":1753328140,"created_at":1751333491},{"id":"web_search","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"Web Search","content":"\"\"\"\ntitle: Web Search using SearXNG and Scrape first N Pages\nauthor: constLiakos with enhancements by justinh-rahb and ther3zz\nfunding_url: https://github.com/open-webui\nversion: 0.1.12\nlicense: MIT\n\"\"\"\n\nimport os\nimport requests\nfrom datetime import datetime\nimport json\nfrom requests import get\nfrom bs4 import BeautifulSoup\nimport concurrent.futures\nfrom html.parser import HTMLParser\nfrom urllib.parse import urlparse, urljoin\nimport re\nimport unicodedata\nfrom pydantic import BaseModel, Field\nimport asyncio\nfrom typing import Callable, Any\n\n\nclass HelpFunctions:\n    def __init__(self):\n        pass\n\n    def get_base_url(self, url):\n        parsed_url = urlparse(url)\n        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n        return base_url\n\n    def generate_excerpt(self, content, max_length=200):\n        return content[:max_length] + \"...\" if len(content) > max_length else content\n\n    def format_text(self, original_text):\n        soup = BeautifulSoup(original_text, \"html.parser\")\n        formatted_text = soup.get_text(separator=\" \", strip=True)\n        formatted_text = unicodedata.normalize(\"NFKC\", formatted_text)\n        formatted_text = re.sub(r\"\\s+\", \" \", formatted_text)\n        formatted_text = formatted_text.strip()\n        formatted_text = self.remove_emojis(formatted_text)\n        return formatted_text\n\n    def remove_emojis(self, text):\n        return \"\".join(c for c in text if not unicodedata.category(c).startswith(\"So\"))\n\n    def process_search_result(self, result, valves):\n        title_site = self.remove_emojis(result[\"title\"])\n        url_site = result[\"url\"]\n        snippet = result.get(\"content\", \"\")\n\n        # Check if the website is in the ignored list, but only if IGNORED_WEBSITES is not empty\n        if valves.IGNORED_WEBSITES:\n            base_url = self.get_base_url(url_site)\n            if any(\n                ignored_site.strip() in base_url\n                for ignored_site in valves.IGNORED_WEBSITES.split(\",\")\n            ):\n                return None\n\n        try:\n            response_site = requests.get(url_site, timeout=20)\n            response_site.raise_for_status()\n            html_content = response_site.text\n\n            soup = BeautifulSoup(html_content, \"html.parser\")\n            content_site = self.format_text(soup.get_text(separator=\" \", strip=True))\n\n            truncated_content = self.truncate_to_n_words(\n                content_site, valves.PAGE_CONTENT_WORDS_LIMIT\n            )\n\n            return {\n                \"title\": title_site,\n                \"url\": url_site,\n                \"content\": truncated_content,\n                \"snippet\": self.remove_emojis(snippet),\n            }\n\n        except requests.exceptions.RequestException as e:\n            return None\n\n    def truncate_to_n_words(self, text, token_limit):\n        tokens = text.split()\n        truncated_tokens = tokens[:token_limit]\n        return \" \".join(truncated_tokens)\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        SEARXNG_ENGINE_API_BASE_URL: str = Field(\n            default=\"https://example.com/search\",\n            description=\"The base URL for Search Engine\",\n        )\n        IGNORED_WEBSITES: str = Field(\n            default=\"\",\n            description=\"Comma-separated list of websites to ignore\",\n        )\n        RETURNED_SCRAPPED_PAGES_NO: int = Field(\n            default=3,\n            description=\"The number of Search Engine Results to Parse\",\n        )\n        SCRAPPED_PAGES_NO: int = Field(\n            default=5,\n            description=\"Total pages scapped. Ideally greater than one of the returned pages\",\n        )\n        PAGE_CONTENT_WORDS_LIMIT: int = Field(\n            default=5000,\n            description=\"Limit words content for each page.\",\n        )\n        CITATION_LINKS: bool = Field(\n            default=False,\n            description=\"If True, send custom citations with links\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n        }\n\n    async def search_web(\n        self,\n        query: str,\n        __event_emitter__: Callable[[dict], Any] = None,\n    ) -> str:\n        \"\"\"\n        Search the web and get the content of the relevant pages. Search for unknown knowledge, news, info, public contact info, weather, etc.\n        :params query: Web Query used in search engine.\n        :return: The content of the pages in json format.\n        \"\"\"\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.emit(f\"Initiating web search for: {query}\")\n\n        search_engine_url = self.valves.SEARXNG_ENGINE_API_BASE_URL\n\n        # Ensure RETURNED_SCRAPPED_PAGES_NO does not exceed SCRAPPED_PAGES_NO\n        if self.valves.RETURNED_SCRAPPED_PAGES_NO > self.valves.SCRAPPED_PAGES_NO:\n            self.valves.RETURNED_SCRAPPED_PAGES_NO = self.valves.SCRAPPED_PAGES_NO\n\n        params = {\n            \"q\": query,\n            \"format\": \"json\",\n            \"number_of_results\": self.valves.RETURNED_SCRAPPED_PAGES_NO,\n        }\n\n        try:\n            await emitter.emit(\"Sending request to search engine\")\n            resp = requests.get(\n                search_engine_url, params=params, headers=self.headers, timeout=120\n            )\n            resp.raise_for_status()\n            data = resp.json()\n\n            results = data.get(\"results\", [])\n            limited_results = results[: self.valves.SCRAPPED_PAGES_NO]\n            await emitter.emit(f\"Retrieved {len(limited_results)} search results\")\n\n        except requests.exceptions.RequestException as e:\n            await emitter.emit(\n                status=\"error\",\n                description=f\"Error during search: {str(e)}\",\n                done=True,\n            )\n            return json.dumps({\"error\": str(e)})\n\n        results_json = []\n        if limited_results:\n            await emitter.emit(f\"Processing search results\")\n\n            with concurrent.futures.ThreadPoolExecutor() as executor:\n                futures = [\n                    executor.submit(\n                        functions.process_search_result, result, self.valves\n                    )\n                    for result in limited_results\n                ]\n                for future in concurrent.futures.as_completed(futures):\n                    result_json = future.result()\n                    if result_json:\n                        try:\n                            json.dumps(result_json)\n                            results_json.append(result_json)\n                        except (TypeError, ValueError):\n                            continue\n                    if len(results_json) >= self.valves.RETURNED_SCRAPPED_PAGES_NO:\n                        break\n\n            results_json = results_json[: self.valves.RETURNED_SCRAPPED_PAGES_NO]\n\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                for result in results_json:\n                    await __event_emitter__(\n                        {\n                            \"type\": \"citation\",\n                            \"data\": {\n                                \"document\": [result[\"content\"]],\n                                \"metadata\": [{\"source\": result[\"url\"]}],\n                                \"source\": {\"name\": result[\"title\"]},\n                            },\n                        }\n                    )\n\n        await emitter.emit(\n            status=\"complete\",\n            description=f\"Web search completed. Retrieved content from {len(results_json)} pages\",\n            done=True,\n        )\n\n        return json.dumps(results_json, ensure_ascii=False)\n\n    async def get_website(\n        self, url: str, __event_emitter__: Callable[[dict], Any] = None\n    ) -> str:\n        \"\"\"\n        Web scrape the website provided and get the content of it.\n        :params url: The URL of the website.\n        :return: The content of the website in json format.\n        \"\"\"\n        functions = HelpFunctions()\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.emit(f\"Fetching content from URL: {url}\")\n\n        results_json = []\n\n        try:\n            response_site = requests.get(url, headers=self.headers, timeout=120)\n            response_site.raise_for_status()\n            html_content = response_site.text\n\n            await emitter.emit(\"Parsing website content\")\n\n            soup = BeautifulSoup(html_content, \"html.parser\")\n\n            page_title = soup.title.string if soup.title else \"No title found\"\n            page_title = unicodedata.normalize(\"NFKC\", page_title.strip())\n            page_title = functions.remove_emojis(page_title)\n            title_site = page_title\n            url_site = url\n            content_site = functions.format_text(\n                soup.get_text(separator=\" \", strip=True)\n            )\n\n            truncated_content = functions.truncate_to_n_words(\n                content_site, self.valves.PAGE_CONTENT_WORDS_LIMIT\n            )\n\n            result_site = {\n                \"title\": title_site,\n                \"url\": url_site,\n                \"content\": truncated_content,\n                \"excerpt\": functions.generate_excerpt(content_site),\n            }\n\n            results_json.append(result_site)\n\n            if self.valves.CITATION_LINKS and __event_emitter__:\n                await __event_emitter__(\n                    {\n                        \"type\": \"citation\",\n                        \"data\": {\n                            \"document\": [truncated_content],\n                            \"metadata\": [{\"source\": url_site}],\n                            \"source\": {\"name\": title_site},\n                        },\n                    }\n                )\n\n            await emitter.emit(\n                status=\"complete\",\n                description=\"Website content retrieved and processed successfully\",\n                done=True,\n            )\n\n        except requests.exceptions.RequestException as e:\n            results_json.append(\n                {\n                    \"url\": url,\n                    \"content\": f\"Failed to retrieve the page. Error: {str(e)}\",\n                }\n            )\n\n            await emitter.emit(\n                status=\"error\",\n                description=f\"Error fetching website content: {str(e)}\",\n                done=True,\n            )\n\n        return json.dumps(results_json, ensure_ascii=False)\n","specs":[{"name":"get_website","description":"Web scrape the website provided and get the content of it.","parameters":{"properties":{"url":{"type":"string"}},"required":["url"],"type":"object"}},{"name":"search_web","description":"Search the web and get the content of the relevant pages. Search for unknown knowledge, news, info, public contact info, weather, etc.","parameters":{"properties":{"query":{"type":"string"}},"required":["query"],"type":"object"}}],"meta":{"description":"Web Search using SearXNG and Scrap first N Pages","manifest":{"title":"Web Search using SearXNG and Scrape first N Pages","author":"constLiakos with enhancements by justinh-rahb and ther3zz","funding_url":"https://github.com/open-webui","version":"0.1.12","license":"MIT"}},"access_control":{},"updated_at":1753328140,"created_at":1751333854},{"id":"crm","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"create_concordcrm_document","content":"import os\nimport requests\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nimport json\n\n\nclass Tools:\n    def __init__(self):\n        pass\n\n    # ... (all your other tools like get_current_time, etc. go here) ...\n\n    def create_concordcrm_document(\n        self,\n        customer_name: str = Field(\n            ...,\n            description=\"The name of the customer for the document title (e.g., 'AssistedVIP').\",\n        ),\n        generated_content_html: str = Field(\n            ...,\n            description=\"The complete HTML content of the Statement of Work generated by the LLM.\",\n        ),\n    ) -> str:\n        \"\"\"\n        Creates a new document in ConcordCRM with the provided SOW HTML content.\n        \"\"\"\n        api_url = \"https://crm.qandu.io/api/documents\"\n        api_token = \"13|gj5aiDChi70vsnK8uqddkFc4TZqaclzgnoqkwyI49ef7d8f1\"\n\n        headers = {\n            \"Authorization\": f\"Bearer {api_token}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n        }\n\n        brand_id = 2\n        # Using the new ID 5 for your \"SOW2\" document type.\n        document_type_id = 5\n        user_id = 1\n\n        payload = {\n            \"title\": f\"{customer_name} Statement of Work\",\n            \"brand_id\": brand_id,\n            \"document_type_id\": document_type_id,\n            \"user_id\": user_id,\n            \"content\": generated_content_html,\n        }\n\n        try:\n            response = requests.post(api_url, headers=headers, json=payload)\n            response.raise_for_status()\n\n            response_data = response.json()\n            document_id = response_data.get(\"data\", {}).get(\"id\")\n\n            if document_id:\n                return f\"Successfully created ConcordCRM document! Document ID: {document_id}. You can view it here: https://crm.qandu.io/documents/{document_id}\"\n            else:\n                return f\"ConcordCRM document created, but could not retrieve ID. API Response: {json.dumps(response_data, indent=2)}\"\n\n        except requests.exceptions.HTTPError as http_err:\n            return (\n                f\"HTTP error occurred: {http_err}. ConcordCRM Response: {response.text}\"\n            )\n        except requests.exceptions.RequestException as req_err:\n            return f\"An unexpected request error occurred: {req_err}.\"\n        except json.JSONDecodeError:\n            return f\"Failed to decode JSON response from ConcordCRM. Raw response: {response.text}\"\n        except Exception as e:\n            return f\"An unexpected error occurred during document creation: {e}\"\n","specs":[{"name":"create_concordcrm_document","description":"Creates a new document in ConcordCRM with the provided SOW HTML content.","parameters":{"properties":{"customer_name":{"description":"The name of the customer for the document title (e.g., 'AssistedVIP').","type":"string"},"generated_content_html":{"description":"The complete HTML content of the Statement of Work generated by the LLM.","type":"string"}},"required":["customer_name","generated_content_html"],"type":"object"}}],"meta":{"description":"create_concordcrm_document","manifest":{}},"access_control":{},"updated_at":1752799730,"created_at":1751971723},{"id":"word_document_generator_clone","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"Word Document Generator (Clone)","content":"import os\nimport re\nimport time\nimport traceback\nimport urllib.request\nimport aiohttp\nfrom docx import Document\nfrom docx.shared import Pt, Inches, RGBColor\nfrom docx.enum.text import WD_PARAGRAPH_ALIGNMENT\nfrom docx.enum.style import WD_STYLE_TYPE\nfrom docx.oxml.ns import nsdecls\nfrom docx.oxml import parse_xml\n\n\nclass Tools:\n    def __init__(self):\n        self.file_handler = True\n\n    def _add_formatted_runs(self, paragraph, text):\n        \"\"\"\n        Clears a paragraph and adds new runs, handling **bold** text.\n        This is the core function for cleaning text.\n        \"\"\"\n        paragraph.clear()\n        parts = re.split(r\"(\\*\\*.*?\\*\\*)\", text)\n        for part in parts:\n            if part.startswith(\"**\") and part.endswith(\"**\"):\n                paragraph.add_run(part[2:-2]).bold = True\n            else:\n                paragraph.add_run(part)\n\n    def _style_modern_table(\n        self, table, brand_color_str=\"10373a\", light_gray_str=\"F2F2F2\"\n    ):\n        \"\"\"Applies a modern, branded style to the given table.\"\"\"\n        header_cells = table.rows[0].cells\n        for cell in header_cells:\n            shading_elm = parse_xml(\n                f'<w:shd {nsdecls(\"w\")} w:fill=\"{brand_color_str}\"/>'\n            )\n            cell._tc.get_or_add_tcPr().append(shading_elm)\n            for p in cell.paragraphs:\n                for run in p.runs:\n                    run.font.color.rgb = RGBColor(0xFF, 0xFF, 0xFF)\n                    run.font.bold = True\n        for i, row in enumerate(table.rows):\n            if i > 0 and i % 2 != 0:\n                shading_elm = parse_xml(\n                    f'<w:shd {nsdecls(\"w\")} w:fill=\"{light_gray_str}\"/>'\n                )\n                for cell in row.cells:\n                    cell._tc.get_or_add_tcPr().append(shading_elm)\n            for cell in row.cells:\n                tcPr = cell._tc.get_or_add_tcPr()\n                tcBorders = parse_xml(\n                    f'<w:tcBorders {nsdecls(\"w\")}><w:bottom w:val=\"single\" w:sz=\"4\" w:space=\"0\" w:color=\"D0D0D0\"/></w:tcBorders>'\n                )\n                tcPr.append(tcBorders)\n                tcMar = parse_xml(\n                    f'<w:tcMar {nsdecls(\"w\")}><w:top w:w=\"100\" w:type=\"dxa\"/><w:bottom w:w=\"100\" w:type=\"dxa\"/><w:left w:w=\"100\" w:type=\"dxa\"/><w:right w:w=\"100\" w:type=\"dxa\"/></w:tcMar>'\n                )\n                tcPr.append(tcMar)\n\n    async def generate_word_doc(self, title: str, content: str) -> dict:\n        safe_title = title.replace(\" \", \"_\").replace(\"/\", \"_\")\n        filename = f\"{safe_title}_{int(time.time())}.docx\"\n        local_path = f\"/tmp/{filename}\"\n        BRAND_COLOR_HEX = \"10373a\"\n        API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjNiOWYwMGZhLTZkYWYtNDk3My1hZGRmLWEwNmJlMzk4Zjk2YiJ9.JK-d1JKXsDCLfLOMJ3SWpRIKC5CLeGpRn2VV0h0HCd0\"\n        PUBLIC_DOWNLOAD_DOMAIN = \"https://socialgarden-ollama-web.vo0egb.easypanel.host\"\n\n        try:\n            doc = Document()\n\n            # --- Style Definitions ---\n            doc.styles[\"Normal\"].font.name = \"Jakarta Sans\"\n            doc.styles[\"Normal\"].font.size = Pt(11)\n            doc.styles[\"Normal\"].paragraph_format.space_after = Pt(8)\n            for i in range(1, 4):\n                style = doc.styles.add_style(\n                    f\"BrandHeading{i}\", WD_STYLE_TYPE.PARAGRAPH\n                )\n                style.base_style = doc.styles[f\"Heading {i}\"]\n                style.font.name = \"Jakarta Sans\"\n                style.font.color.rgb = RGBColor.from_string(BRAND_COLOR_HEX)\n                style.paragraph_format.space_before = Pt(18)\n                style.paragraph_format.space_after = Pt(6)\n            doc.styles[\"BrandHeading1\"].font.size = Pt(22)\n            doc.styles[\"BrandHeading2\"].font.size = Pt(16)\n            doc.styles[\"BrandHeading3\"].font.size = Pt(12)\n            doc.styles[\"BrandHeading3\"].font.italic = True\n            bullet_style = doc.styles.add_style(\"BrandBullet\", WD_STYLE_TYPE.PARAGRAPH)\n            bullet_style.base_style = doc.styles[\"List Bullet\"]\n            bullet_style.font.name = \"Jakarta Sans\"\n            bullet_style.paragraph_format.space_after = Pt(4)\n\n            # --- Document Creation ---\n            logo_path = \"/tmp/qandu_logo.png\"\n            if not os.path.exists(logo_path):\n                urllib.request.urlretrieve(\n                    \"https://i.postimg.cc/qRYg61Kq/Logo-Dark-Green.png\", logo_path\n                )\n            doc.add_picture(logo_path, width=Inches(2.5))\n            doc.paragraphs[-1].alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n            doc.add_heading(title, level=0)\n\n            # --- Content Parsing ---\n            lines = content.split(\"\\n\")\n            is_in_table = False\n            table_data = []\n\n            for i, line in enumerate(lines):\n                line = line.strip()\n                if not line:\n                    continue\n\n                # Table Parsing\n                if line.startswith(\"|\") and line.endswith(\"|\"):\n                    if not is_in_table:\n                        is_in_table = True\n                        table_data = []\n                    if \"---\" in line:\n                        continue\n                    cells = [cell.strip() for cell in line.split(\"|\")[1:-1]]\n                    table_data.append(cells)\n                    if (i + 1 == len(lines)) or not (\n                        lines[i + 1].strip().startswith(\"|\")\n                    ):\n                        if len(table_data) > 1:\n                            table = doc.add_table(\n                                rows=len(table_data), cols=len(table_data[0])\n                            )\n                            table.autofit = True\n                            for row_idx, row_data in enumerate(table_data):\n                                for col_idx, cell_text in enumerate(row_data):\n                                    cell = table.cell(row_idx, col_idx)\n                                    self._add_formatted_runs(\n                                        cell.paragraphs[0], cell_text\n                                    )\n                                    cell.paragraphs[0].alignment = (\n                                        WD_PARAGRAPH_ALIGNMENT.CENTER\n                                    )\n                            self._style_modern_table(\n                                table, brand_color_str=BRAND_COLOR_HEX\n                            )\n                            doc.add_paragraph()\n                        is_in_table = False\n                        table_data = []\n                    continue\n\n                # Text Formatting\n                if line.startswith(\"### \"):\n                    p = doc.add_paragraph(style=\"BrandHeading3\")\n                    self._add_formatted_runs(p, line[4:])\n                elif line.startswith(\"## \"):\n                    p = doc.add_paragraph(style=\"BrandHeading2\")\n                    self._add_formatted_runs(p, line[3:])\n                elif line.startswith(\"# \"):\n                    p = doc.add_paragraph(style=\"BrandHeading1\")\n                    self._add_formatted_runs(p, line[2:])\n                elif line.startswith(\"- \"):\n                    p = doc.add_paragraph(style=\"BrandBullet\")\n                    self._add_formatted_runs(p, line[2:])\n                else:\n                    p = doc.add_paragraph(style=\"Normal\")\n                    self._add_formatted_runs(p, line)\n\n            doc.save(local_path)\n\n        except Exception as e:\n            print(traceback.format_exc())\n            return {\"error\": f\"❌ Failed to generate DOCX: {e}\"}\n\n        # --- Upload Logic ---\n        try:\n            headers = {\n                \"Authorization\": f\"Bearer {API_KEY}\",\n                \"Accept\": \"application/json\",\n            }\n            with open(local_path, \"rb\") as f:\n                form = aiohttp.FormData()\n                form.add_field(\n                    \"file\",\n                    f,\n                    filename=filename,\n                    content_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n                )\n                async with aiohttp.ClientSession(\n                    timeout=aiohttp.ClientTimeout(total=30)\n                ) as session:\n                    async with session.post(\n                        f\"{PUBLIC_DOWNLOAD_DOMAIN}/api/v1/files/\",\n                        headers=headers,\n                        data=form,\n                    ) as resp:\n                        status = resp.status\n                        if status >= 200 and status < 300:\n                            data = await resp.json()\n                            file_id = (\n                                data.get(\"id\")\n                                or data.get(\"uuid\")\n                                or data.get(\"file_id\")\n                            )\n                            if file_id:\n                                return {\n                                    \"url\": f\"{PUBLIC_DOWNLOAD_DOMAIN}/api/v1/files/{file_id}/content\"\n                                }\n                            return {\"error\": \"❌ No file ID in response\", \"json\": data}\n                        text = await resp.text()\n                        return {\"error\": f\"❌ HTTP {status}\", \"raw_response\": text}\n        except Exception as e:\n            print(traceback.format_exc())\n            return {\"error\": f\"❌ Upload failed: {e}\"}\n        finally:\n            if os.path.exists(local_path):\n                os.remove(local_path)\n","specs":[{"name":"_add_formatted_runs","description":"Clears a paragraph and adds new runs, handling **bold** text.\nThis is the core function for cleaning text.","parameters":{"properties":{"paragraph":{},"text":{}},"required":["paragraph","text"],"type":"object"}},{"name":"_style_modern_table","description":"Applies a modern, branded style to the given table.","parameters":{"properties":{"table":{},"brand_color_str":{"default":"10373a"},"light_gray_str":{"default":"F2F2F2"}},"required":["table"],"type":"object"}},{"name":"generate_word_doc","description":"","parameters":{"properties":{"title":{"type":"string"},"content":{"type":"string"}},"required":["title","content"],"type":"object"}}],"meta":{"description":"Generates a word document from a given prompt. Provides download url for it in the chat. ","manifest":{}},"access_control":{},"updated_at":1752799730,"created_at":1751987620},{"id":"youtube2","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"youtube2","content":"\"\"\"\ntitle: Youtube Transcript Provider (Langchain Community)\nauthor: thearyadev\nauthor_url: https://github.com/thearyadev/youtube-transcript-provider\nfunding_url: https://github.com/open-webui\nversion: 0.0.3\n\"\"\"\n\nfrom typing import Awaitable, Callable, Any\nfrom langchain_community.document_loaders import YoutubeLoader\nfrom langchain_community.document_loaders.youtube import TranscriptFormat\nimport traceback\nimport asyncio\n\n\nclass Tools:\n    def __init__(self):\n        self.citation = True\n\n    async def get_youtube_transcript(\n        self,\n        url: str,\n        __event_emitter__: Callable[[dict[str, dict[str, Any] | str]], Awaitable[None]],\n    ) -> str:\n        \"\"\"\n        Provides the title and full transcript of a YouTube video in English.\n        Only use if the user supplied a valid YouTube URL.\n        Examples of valid YouTube URLs: https://youtu.be/dQw4w9WgXcQ, https://www.youtube.com/watch?v=dQw4w9WgXcQ\n\n        :param url: The URL of the youtube video that you want the transcript for.\n        :return: The title and full transcript of the YouTube video in English, or an error message.\n        \"\"\"\n        try:\n            if \"dQw4w9WgXcQ\" in url:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": f\"{url} is not a valid youtube link\",\n                            \"done\": True,\n                        },\n                    }\n                )\n                return \"The tool failed with an error. No transcript has been provided.\"\n\n            data = YoutubeLoader.from_youtube_url(\n                # video info seems to be broken\n                youtube_url=url,\n                add_video_info=False,\n                language=[\"en\", \"en_auto\"],\n                transcript_format=TranscriptFormat.CHUNKS,\n                chunk_size_seconds=30,\n            ).load()\n\n            if not data:\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": f\"Failed to retrieve transcript for {url}. No results\",\n                            \"done\": True,\n                        },\n                    }\n                )\n                return \"The tool failed with an error. No transcript has been provided.\"\n\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": f\"Successfully retrieved transcript for {url}\",\n                        \"done\": True,\n                    },\n                }\n            )\n\n            n = \"\\n\"\n\n            text_out = \"\"\n            for chunk in data:\n                text_out += f\"[{chunk.metadata['start_timestamp']}] {chunk.page_content.replace(n, '')}{n}\"\n\n            print(text_out)\n\n            return text_out\n        except:\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": f\"Failed to retrieve transcript for {url}.\",\n                        \"done\": True,\n                    },\n                }\n            )\n            return f\"The tool failed with an error. No transcript has been provided.\\nError Traceback: \\n{traceback.format_exc()}\"\n\n\nasync def event_emitter(*args, **kwargs) -> None:\n    return None\n\n\nif __name__ == \"__main__\":\n    asyncio.run(Tools().get_youtube_transcript(\"https://www.youtube.com/watch?v=YIoBxXc1xtU\", __event_emitter__=event_emitter))  # type: ignore\n","specs":[{"name":"get_youtube_transcript","description":"Provides the title and full transcript of a YouTube video in English.\nOnly use if the user supplied a valid YouTube URL.\nExamples of valid YouTube URLs: https://youtu.be/dQw4w9WgXcQ, https://www.youtube.com/watch?v=dQw4w9WgXcQ","parameters":{"properties":{"url":{"description":"The URL of the youtube video that you want the transcript for.","type":"string"}},"required":["url"],"type":"object"}}],"meta":{"description":"y","manifest":{"title":"Youtube Transcript Provider (Langchain Community)","author":"thearyadev","author_url":"https://github.com/thearyadev/youtube-transcript-provider","funding_url":"https://github.com/open-webui","version":"0.0.3"}},"access_control":{},"updated_at":1752799730,"created_at":1752607168},{"id":"pptt","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"PPTist Slide Editor","content":"\"\"\"\ntitle: PPTist Slide Editor\nauthor: PPTist Developer\nauthor_url: https://github.com/pipipi-pikachu/PPTist\ngit_url: https://github.com/pipipi-pikachu/PPTist\ndescription: Opens the PPTist editor in a new browser tab via a plain text link from Open WebUI\nrequired_open_webui_version: 0.4.0\nrequirements:\nversion: 1.0.0\nlicence: MIT\n\"\"\"\n\n\nclass Tools:\n    def __init__(self):\n        self.pptist_url = \"https://ahmad-ppt.vo0egb.easypanel.host/\"\n\n    async def open_pptist_link(self, __event_emitter__=None) -> str:\n        \"\"\"\n        Emit a clickable link from Open WebUI to open the PPTist slide editor\n        \"\"\"\n        if __event_emitter__:\n            await __event_emitter__(\n                {\n                    \"type\": \"message\",\n                    \"data\": {\n                        \"content\": (\n                            f\"👉 **[Click here to open the PPTist Slide Editor]({self.pptist_url})**\\n\\n\"\n                            f\"If it doesn't open, make sure the PPTist service is running and port 5173 is exposed in Easypanel.\"\n                        )\n                    },\n                }\n            )\n        return \"✅ Link generated. Please open the PPTist editor in a new browser tab.\"\n","specs":[{"name":"open_pptist_link","description":"Emit a clickable link from Open WebUI to open the PPTist slide editor","parameters":{"properties":{},"type":"object"}}],"meta":{"description":"f","manifest":{"title":"PPTist Slide Editor","author":"PPTist Developer","author_url":"https://github.com/pipipi-pikachu/PPTist","git_url":"https://github.com/pipipi-pikachu/PPTist","description":"Opens the PPTist editor in a new browser tab via a plain text link from Open WebUI","required_open_webui_version":"0.4.0","requirements":"","version":"1.0.0","licence":"MIT"}},"access_control":{},"updated_at":1752799730,"created_at":1752674678},{"id":"ppt_gem","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"ppt gem","content":"\"\"\"\ntitle: Presentation Generator (Final Fix)\nauthor: Ahmad\nauthor_url:\ngit_url:\ndescription: To create a presentation, you must call the function `generate_presentation_file`. It takes a `topic` and `num_slides`. This is the only tool for making presentations.\nrequired_open_webui_version: 0.4.0\nrequirements:\nversion: 2.5.0\nlicence: MIT\n\"\"\"\n\nimport os\nimport requests\nimport subprocess\nimport uuid\nimport re\nimport urllib.parse\nimport json\nimport shutil\n\n\nclass Tools:\n    def __init__(self):\n        # --- CONFIGURATION (All values are pre-filled) ---\n        self.open_webui_api_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjNiOWYwMGZhLTZkYWYtNDk3My1hZGRmLWEwNmJlMzk4Zjk2YiJ9.JK-d1JKXsDCLfLOMJ3SWpRIKC5CLeGpRn2VV0h0HCd0\"\n        self.open_webui_api_url = (\n            \"https://socialgarden-ollama-web.vo0egb.easypanel.host/v1/chat/completions\"\n        )\n        self.output_dir = \"/app/backend/data/downloads\"\n        self.base_url = (\n            \"https://socialgarden-ollama-web.vo0egb.easypanel.host/downloads\"\n        )\n        # --- END OF CONFIGURATION ---\n\n        if not os.path.exists(self.output_dir):\n            try:\n                os.makedirs(self.output_dir)\n            except OSError as e:\n                print(f\"Error creating directory {self.output_dir}: {e}\")\n\n    async def generate_presentation_file(\n        self,\n        topic: str,\n        num_slides: int = 5,\n        __body__: dict = {},\n        __event_emitter__=None,\n    ) -> str:\n        \"\"\"\n        Generates a complete presentation from a single topic.\n\n        Args:\n            topic (str): The subject of the presentation (e.g., \"the history of the internet\").\n            num_slides (int): The number of slides to generate. Defaults to 5.\n            __body__ (dict): The full request body from Open WebUI, used to get the current model.\n        \"\"\"\n        print(\"--- CORRECT TOOL CALLED: generate_presentation_file ---\")\n\n        if not shutil.which(\"marp\"):\n            return \"❌ **Critical Error:** The `marp` command was not found on the server.\\n\\nPlease ensure Marp CLI is installed globally inside your Open WebUI container by running:\\n`npm install -g @marp-team/marp-cli`\"\n\n        current_model = __body__.get(\"model\", None)\n        if not current_model:\n            return \"❌ **Critical Error:** Could not automatically detect the model from your current chat session. Please try starting a new chat and running the tool again.\"\n\n        session_id = str(uuid.uuid4())\n        html_filename = f\"presentation-{session_id}.html\"\n        pptx_filename = f\"presentation-{session_id}.pptx\"\n        md_filepath = os.path.join(self.output_dir, f\"temp-{session_id}.md\")\n        html_filepath = os.path.join(self.output_dir, html_filename)\n        pptx_filepath = os.path.join(self.output_dir, pptx_filename)\n\n        await __event_emitter__(\n            {\n                \"type\": \"message\",\n                \"data\": {\n                    \"content\": f\"🚀 **Starting {num_slides}-slide presentation on '{topic}'...**\\n✍️ Using model `{current_model}` to write the content.\"\n                },\n            }\n        )\n\n        prompt = f\"\"\"\nYou are an expert presentation designer. Create a {num_slides}-slide presentation about \"{topic}\".\nThe output must be in valid Marp Markdown format. Each slide must be separated by `---`.\nThe first slide must be a title slide with `marp: true` and a theme like 'uncover' or 'gaia'.\nFor every slide, including the title slide, you MUST include a background image using this exact syntax: `![bg](search: a descriptive search query for a relevant background image)`.\nThe content should be well-structured with headings and bullet points. Do not include any text or explanation outside of the markdown code.\n\"\"\"\n\n        headers = {\n            \"Authorization\": f\"Bearer {self.open_webui_api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        payload = {\n            \"model\": current_model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"stream\": False,\n        }\n\n        try:\n            await __event_emitter__(\n                {\n                    \"type\": \"message\",\n                    \"data\": {\"content\": \"➡️ Sending prompt to the AI model...\"},\n                }\n            )\n            response = requests.post(\n                self.open_webui_api_url,\n                headers=headers,\n                data=json.dumps(payload),\n                timeout=60,\n            )\n            response.raise_for_status()\n\n            await __event_emitter__(\n                {\n                    \"type\": \"message\",\n                    \"data\": {\"content\": \"✅ AI response received. Parsing content...\"},\n                }\n            )\n            response_data = response.json()\n            if \"choices\" not in response_data or not response_data.get(\"choices\"):\n                return (\n                    \"❌ **Error:** The AI model returned an empty or invalid response.\"\n                )\n\n            markdown_content = response_data[\"choices\"][0][\"message\"][\"content\"]\n            markdown_content = (\n                markdown_content.replace(\"```markdown\", \"\").replace(\"```\", \"\").strip()\n            )\n\n        except requests.exceptions.Timeout:\n            return \"❌ **Error:** The request to the AI model timed out. The model might be offline or taking too long to respond.\"\n        except requests.exceptions.RequestException as e:\n            return f\"❌ **Error:** Failed to contact the AI model. Details: {e}\"\n        except (json.JSONDecodeError, KeyError, IndexError) as e:\n            return f\"❌ **Error:** Could not understand the AI's response. It might not be valid. Details: {e}\"\n\n        await __event_emitter__(\n            {\n                \"type\": \"message\",\n                \"data\": {\"content\": \"🖼️ Finding images on Pollinations.ai...\"},\n            }\n        )\n\n        final_markdown = markdown_content\n        search_tags = re.findall(r\"!\\[bg\\]\\(search:\\s*(.*?)\\)\", final_markdown)\n\n        for query in search_tags:\n            encoded_query = urllib.parse.quote_plus(query)\n            image_url = f\"https://image.pollinations.ai/prompt/{encoded_query}?width=1280&height=1024&nologo=true\"\n            final_markdown = final_markdown.replace(\n                f\"![bg](search: {query})\", f\"![bg]({image_url})\", 1\n            )\n\n        await __event_emitter__(\n            {\n                \"type\": \"message\",\n                \"data\": {\"content\": \"⚙️ Building presentation files with Marp...\"},\n            }\n        )\n\n        try:\n            with open(md_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(final_markdown)\n        except IOError as e:\n            return f\"❌ **Error:** Could not write temporary file to server path '{md_filepath}'. Check permissions. Details: {e}\"\n\n        try:\n            subprocess.run(\n                [\"marp\", md_filepath, \"--html\", \"-o\", html_filepath],\n                check=True,\n                capture_output=True,\n                text=True,\n                timeout=120,\n            )\n            subprocess.run(\n                [\"marp\", md_filepath, \"--pptx\", \"-o\", pptx_filepath],\n                check=True,\n                capture_output=True,\n                text=True,\n                timeout=120,\n            )\n        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\n            error_message = e.stderr if hasattr(e, \"stderr\") else str(e)\n            return f\"❌ **Error:** Failed to create presentation files. Marp Error:\\n`{error_message}`\"\n        finally:\n            if os.path.exists(md_filepath):\n                os.remove(md_filepath)\n\n        html_url = f\"{self.base_url}/{html_filename}\"\n        pptx_url = f\"{self.base_url}/{pptx_filename}\"\n\n        return (\n            f\"✅ **Presentation Ready!**\\n\\n\"\n            f\"👉 **[Click here to view your presentation in your browser]({html_url})**\\n\\n\"\n            f\"You can also [download the PowerPoint file directly]({pptx_url}).\"\n        )\n","specs":[{"name":"generate_presentation_file","description":"Generates a complete presentation from a single topic.\n\nArgs:\ntopic (str): The subject of the presentation (e.g., \"the history of the internet\").\nnum_slides (int): The number of slides to generate. Defaults to 5.\n__body__ (dict): The full request body from Open WebUI, used to get the current model.","parameters":{"properties":{"topic":{"type":"string"},"num_slides":{"default":5,"type":"integer"}},"required":["topic"],"type":"object"}}],"meta":{"description":"p","manifest":{"title":"Presentation Generator (Final Fix)","author":"Ahmad","author_url":"","git_url":"","description":"To create a presentation, you must call the function `generate_presentation_file`. It takes a `topic` and `num_slides`. This is the only tool for making presentations.","required_open_webui_version":"0.4.0","requirements":"","version":"2.5.0","licence":"MIT"}},"access_control":{},"updated_at":1752799730,"created_at":1752676297},{"id":"youtube_transcript_provider","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"Youtube Transcript Provider","content":"\"\"\"\ntitle: Youtube Transcript Provider\nauthor: ekatiyar\nauthor_url: https://github.com/ekatiyar\ngit_url: https://github.com/ekatiyar/open-webui-tools\ndescription: A tool that returns the full youtube transcript in English of a passed in youtube url.\nrequirements: langchain-yt-dlp\nversion: 0.0.8\nlicense: MIT\n\"\"\"\n\nimport unittest\nfrom typing import Any, Callable\n\nfrom langchain_community.document_loaders import YoutubeLoader\nfrom langchain_yt_dlp.youtube_loader import YoutubeLoaderDL\nfrom pydantic import BaseModel, Field\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def progress_update(self, description):\n        await self.emit(description)\n\n    async def error_update(self, description):\n        await self.emit(description, \"error\", True)\n\n    async def success_update(self, description):\n        await self.emit(description, \"success\", True)\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        CITITATION: bool = Field(\n            default=\"True\", description=\"True or false for citation\"\n        )\n\n    class UserValves(BaseModel):\n        TRANSCRIPT_LANGUAGE: str = Field(\n            default=\"en,en_auto\",\n            description=\"A comma-separated list of languages from highest priority to lowest.\",\n        )\n        TRANSCRIPT_TRANSLATE: str = Field(\n            default=\"en\",\n            description=\"The language you want the transcript to auto-translate to, if it does not already exist.\",\n        )\n        GET_VIDEO_DETAILS: bool = Field(\n            default=\"True\", description=\"Grab video details, such as title and author\"\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.citation = self.valves.CITITATION\n\n    async def get_youtube_transcript(\n        self,\n        url: str,\n        __event_emitter__: Callable[[dict], Any] = None,\n        __user__: dict = {},\n    ) -> str:\n        \"\"\"\n        Provides the title and full transcript of a YouTube video in English.\n        Only use if the user supplied a valid YouTube URL.\n        Examples of valid YouTube URLs: https://youtu.be/dQw4w9WgXcQ, https://www.youtube.com/watch?v=dQw4w9WgXcQ\n\n        :param url: The URL of the youtube video that you want the transcript for.\n        :return: The full transcript of the YouTube video in English, or an error message.\n        \"\"\"\n        emitter = EventEmitter(__event_emitter__)\n        if \"valves\" not in __user__:\n            __user__[\"valves\"] = self.UserValves()\n\n        try:\n            await emitter.progress_update(f\"Validating URL: {url}\")\n\n            # Check if the URL is valid\n            if not url or url == \"\":\n                raise Exception(f\"Invalid YouTube URL: {url}\")\n            # LLM's love passing in this url when the user doesn't provide one\n            elif \"dQw4w9WgXcQ\" in url:\n                raise Exception(\"Rick Roll URL provided... is that what you want?).\")\n\n            # Get video details if the user wants them\n            title = \"\"\n            author = \"\"\n            if __user__[\"valves\"].GET_VIDEO_DETAILS:\n                await emitter.progress_update(\"Getting video details\")\n                details = await YoutubeLoaderDL.from_youtube_url(\n                    url, add_video_info=True\n                ).aload()\n\n                if len(details) == 0:\n                    raise Exception(\"Failed to get video details\")\n\n                title = details[0].metadata[\"title\"]\n                author = details[0].metadata[\"author\"]\n                await emitter.progress_update(\n                    f\"Grabbed details for {title} by {author}\"\n                )\n\n            languages = [\n                item.strip()\n                for item in __user__[\"valves\"].TRANSCRIPT_LANGUAGE.split(\",\")\n            ]\n\n            transcript = await YoutubeLoader.from_youtube_url(\n                url,\n                add_video_info=False,\n                language=languages,\n                translation=__user__[\"valves\"].TRANSCRIPT_TRANSLATE,\n            ).aload()\n\n            if len(transcript) == 0:\n                raise Exception(\n                    f\"Failed to find transcript for {title if title else url}\"\n                )\n\n            transcript = \"\\n\".join([document.page_content for document in transcript])\n\n            if title and author:\n                transcript = f\"{title}\\nby {author}\\n\\n{transcript}\"\n\n            await emitter.success_update(f\"Transcript for video {title} retrieved!\")\n            return transcript\n\n        except Exception as e:\n            error_message = f\"Error: {str(e)}\"\n            await emitter.error_update(error_message)\n            return error_message\n\n\nclass YoutubeTranscriptProviderTest(unittest.IsolatedAsyncioTestCase):\n    async def assert_transcript_length(self, url: str, expected_length: int):\n        self.assertEqual(\n            len(await Tools().get_youtube_transcript(url)), expected_length\n        )\n\n    async def assert_transcript_error(self, url: str):\n        response = await Tools().get_youtube_transcript(url)\n        self.assertTrue(\"Error\" in response)\n\n    async def test_get_youtube_transcript(self):\n        url = \"https://www.youtube.com/watch?v=zhWDdy_5v2w\"\n        await self.assert_transcript_length(url, 1380)\n\n    async def test_get_youtube_transcript_with_invalid_url(self):\n        invalid_url = \"https://www.example.com/invalid\"\n        missing_url = \"https://www.youtube.com/watch?v=zhWDdy_5v3w\"\n        rick_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n\n        await self.assert_transcript_error(invalid_url)\n        await self.assert_transcript_error(missing_url)\n        await self.assert_transcript_error(rick_url)\n\n    async def test_get_youtube_transcript_with_none_arg(self):\n        await self.assert_transcript_error(None)\n        await self.assert_transcript_error(\"\")\n\n\nif __name__ == \"__main__\":\n    print(\"Running tests...\")\n    unittest.main()\n","specs":[{"name":"get_youtube_transcript","description":"Provides the title and full transcript of a YouTube video in English.\nOnly use if the user supplied a valid YouTube URL.\nExamples of valid YouTube URLs: https://youtu.be/dQw4w9WgXcQ, https://www.youtube.com/watch?v=dQw4w9WgXcQ","parameters":{"properties":{"url":{"description":"The URL of the youtube video that you want the transcript for.","type":"string"}},"required":["url"],"type":"object"}}],"meta":{"description":"A tool that returns the full youtube transcript in English for a given youtube video url instead of using RAG","manifest":{"title":"Youtube Transcript Provider","author":"ekatiyar","author_url":"https://github.com/ekatiyar","git_url":"https://github.com/ekatiyar/open-webui-tools","description":"A tool that returns the full youtube transcript in English of a passed in youtube url.","requirements":"langchain-yt-dlp","version":"0.0.8","license":"MIT"}},"access_control":{},"updated_at":1752606796,"created_at":1751333552},{"id":"ppddff","user_id":"3b9f00fa-6daf-4973-addf-a06be398f96b","name":"ppddff","content":"import os\nimport time\nimport traceback\nimport aiohttp\n\nAPI_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjNiOWYwMGZhLTZkYWYtNDk3My1hZGRmLWEwNmJlMzk4Zjk2YiJ9.JK-d1JKXsDCLfLOMJ3SWpRIKC5CLeGpRn2VV0h0HCd0\"\nPUBLIC_DOWNLOAD_DOMAIN = \"https://socialgarden-ollama-web.vo0egb.easypanel.host\"\n\n\nclass Tools:\n    def __init__(self):\n        self.file_handler = True\n\n    async def ppddff(self, html_content: str) -> dict:\n        \"\"\"Generate branded PDF from HTML content\"\"\"\n        try:\n            current_year = time.strftime(\"%Y\")\n            branded_html = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>PPDDFF Document</title>\n    <style>\n        @import url('https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700&display=swap');\n        body {{\n            font-family: 'Plus Jakarta Sans', sans-serif;\n            margin: 0;\n            padding: 0;\n            color: #10373A;\n            line-height: 1.6;\n        }}\n        .header {{\n            text-align: center;\n            padding: 20px 0;\n            border-bottom: 1px solid #10373A;\n            margin-bottom: 30px;\n        }}\n        .header img {{\n            max-width: 200px;\n            height: auto;\n        }}\n        .content {{\n            padding: 0 40px;\n            margin-bottom: 60px;\n        }}\n        .footer {{\n            background-color: #10373A;\n            color: white;\n            text-align: center;\n            padding: 20px;\n            position: fixed;\n            bottom: 0;\n            width: 100%;\n        }}\n        .footer img {{\n            max-width: 150px;\n            height: auto;\n            margin-top: 10px;\n        }}\n        h1, h2, h3 {{\n            color: #10373A;\n            font-weight: 600;\n        }}\n        p {{\n            margin-bottom: 15px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <img src=\"https://i.postimg.cc/025BspNV/Logo-Dark-Green.png\" alt=\"Logo\">\n    </div>\n    <div class=\"content\">\n        {html_content}\n    </div>\n    <div class=\"footer\">\n        <p>© {current_year} PPDDFF. All rights reserved.</p>\n        <img src=\"https://i.postimg.cc/W3Ks0nBx/Social-Garden.png\" alt=\"Social Garden Logo\">\n    </div>\n</body>\n</html>\"\"\"\n\n            pdf_server_url = \"http://168.231.115.219:5001/generate_pdf\"\n\n            async with aiohttp.ClientSession(\n                timeout=aiohttp.ClientTimeout(total=30)\n            ) as session:\n                form = aiohttp.FormData()\n                form.add_field(\"html\", branded_html)\n\n                async with session.post(pdf_server_url, data=form) as resp:\n                    if resp.status != 200:\n                        return {\"error\": f\"❌ PDF server error: {resp.status}\"}\n\n                    pdf_bytes = await resp.read()\n                    filename = f\"ppddff_{int(time.time())}.pdf\"\n                    local_path = f\"/tmp/{filename}\"\n\n                    with open(local_path, \"wb\") as f:\n                        f.write(pdf_bytes)\n\n                    headers = {\n                        \"Authorization\": f\"Bearer {API_KEY}\",\n                        \"Accept\": \"application/json\",\n                    }\n                    upload_form = aiohttp.FormData()\n                    upload_form.add_field(\n                        \"file\",\n                        open(local_path, \"rb\"),\n                        filename=filename,\n                        content_type=\"application/pdf\",\n                    )\n\n                    async with session.post(\n                        f\"{PUBLIC_DOWNLOAD_DOMAIN}/api/v1/files/\",\n                        headers=headers,\n                        data=upload_form,\n                    ) as upload_resp:\n                        data = await upload_resp.json()\n\n                    os.remove(local_path)\n                    file_id = data.get(\"id\") or data.get(\"uuid\") or data.get(\"file_id\")\n                    download_url = (\n                        f\"{PUBLIC_DOWNLOAD_DOMAIN}/api/v1/files/{file_id}/content\"\n                    )\n                    return {\"url\": download_url}\n\n        except Exception as e:\n            return {\"error\": f\"❌ Failed: {e}\"}\n","specs":[{"name":"ppddff","description":"Generate branded PDF from HTML content","parameters":{"properties":{"html_content":{"type":"string"}},"required":["html_content"],"type":"object"}}],"meta":{"description":"ppddff","manifest":{}},"access_control":{},"updated_at":1752341571,"created_at":1752337930},{"id":"deepresearch","user_id":"76bb3965-6767-4daa-810e-ae33b4628a48","name":"DeepResearch","content":"\"\"\"\ntitle: BrowserUI Deep Research\nauthor: JAYADEEP V\nauthor_url: https://github.com/jaideepv # Or your repo if applicable\ngit_url: https://github.com/open-webui/open-webui # Or your repo if applicable\ndescription: Performs deep web research using a BrowserUI (e.g., running on localhost:7788) instance via its Gradio API, providing streaming status updates.\nrequired_open_webui_version: 0.4.0 # Requires event emitters and async tools\nrequirements: requests\nversion: 0.1.0\nlicense: MIT\n\"\"\"\n\nimport requests\nimport json\nimport logging\nimport os\nfrom typing import Callable, Any, Optional\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime  # Needed for potential future citation use\n\n# Configure basic logging for the tool\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - [BrowserUITool] - %(message)s\",\n)\n\n\nclass Tools:\n\n    # Valves: Configurable by Admins in Open WebUI settings\n    class Valves(BaseModel):\n        BROWSERUI_URL: str = Field(\n            default=\"http://localhost:7788\",\n            description=\"Base URL for the BrowserUI Gradio API (e.g., http://localhost:7788)\",\n        )\n        LLM_PROVIDER: str = Field(\n            default=\"openai\",\n            description=\"Default LLM provider to use in BrowserUI (e.g., 'openai', 'ollama', 'anthropic'). Must be supported by BrowserUI.\",\n        )\n        LLM_MODEL_NAME: str = Field(\n            default=\"gpt-4o\",\n            description=\"Default LLM model name to use in BrowserUI (e.g., 'gpt-4o', 'llama3'). Must be supported by BrowserUI and the provider.\",\n        )\n        # Optional API Key Valve - Admins can set this, otherwise environment variables are checked\n        API_KEY: Optional[str] = Field(\n            default=None,\n            description=\"[Optional] API Key for the selected LLM Provider. If unset, the tool will try common environment variables (e.g., OPENAI_API_KEY).\",\n        )\n        MAX_SEARCH_ITERATIONS: int = Field(\n            default=3,\n            description=\"Default maximum search iterations for the deep research agent.\",\n        )\n        MAX_QUERY_PER_ITER: int = Field(\n            default=1, description=\"Default maximum queries per search iteration.\"\n        )\n        LLM_TEMPERATURE: float = Field(\n            default=0.6,\n            description=\"Default temperature for the LLM used by BrowserUI.\",\n        )\n        LLM_NUM_CTX: int = Field(\n            default=16000,\n            description=\"Default context length (primarily for Ollama models in BrowserUI).\",\n        )\n        LLM_BASE_URL: str = Field(\n            default=\"\",\n            description=\"[Optional] Custom Base URL for the LLM API endpoint if needed by BrowserUI.\",\n        )\n        USE_VISION: bool = Field(\n            default=True,\n            description=\"Allow the BrowserUI agent to use vision capabilities (if model supports).\",\n        )\n        HEADLESS_BROWSER: bool = Field(\n            default=False,  # Default to non-headless for easier debugging if needed\n            description=\"Run the BrowserUI browser in headless mode.\",\n        )\n        REQUEST_TIMEOUT: int = Field(\n            default=60,\n            description=\"Timeout in seconds for the initial request to start the research task.\",\n        )\n\n    def __init__(self):\n        \"\"\"Initialize the Tool and its valves.\"\"\"\n        self.valves = self.Valves()\n        # If you were implementing custom citations, you would set self.citation = False here\n        # self.citation = False\n        logging.info(\"BrowserUI Deep Research Tool initialized.\")\n\n    async def perform_deep_research(\n        self, research_task: str, __event_emitter__: Callable[[dict], Any]\n    ) -> str:\n        \"\"\"\n        Performs deep research on a given topic using an external BrowserUI instance.\n\n        :param research_task: The detailed research task or question.\n        :param __event_emitter__: Open WebUI event emitter for status updates.\n        :return: The final research report string or an error message.\n        \"\"\"\n        logging.info(f\"Starting deep research task: '{research_task[:100]}...'\")\n        await __event_emitter__(\n            {\n                \"type\": \"status\",\n                \"data\": {\n                    \"description\": \"Initiating deep research request to BrowserUI...\",\n                    \"done\": False,\n                    \"hidden\": False,\n                },\n            }\n        )\n\n        # --- Get configuration from valves ---\n        api_host = self.valves.BROWSERUI_URL.rstrip(\"/\")\n        llm_provider = self.valves.LLM_PROVIDER\n        llm_model_name = self.valves.LLM_MODEL_NAME\n        llm_temperature = self.valves.LLM_TEMPERATURE\n        llm_num_ctx = self.valves.LLM_NUM_CTX\n        llm_base_url = self.valves.LLM_BASE_URL\n        use_vision = self.valves.USE_VISION\n        headless = self.valves.HEADLESS_BROWSER\n        max_search_iterations = self.valves.MAX_SEARCH_ITERATIONS\n        max_query_per_iter = self.valves.MAX_QUERY_PER_ITER\n        request_timeout = self.valves.REQUEST_TIMEOUT\n\n        # --- API Key Handling: Prioritize Valve, then Env Vars ---\n        effective_api_key = self.valves.API_KEY  # Check valve first\n\n        if not effective_api_key:\n            # Try environment variables if valve not set\n            env_var_map = {\n                \"openai\": \"OPENAI_API_KEY\",\n                \"anthropic\": \"ANTHROPIC_API_KEY\",\n                \"google\": \"GOOGLE_API_KEY\",\n                \"azure_openai\": \"AZURE_OPENAI_API_KEY\",\n                \"mistral\": \"MISTRAL_API_KEY\",  # Add others as needed\n            }\n            env_key = env_var_map.get(llm_provider)\n            if env_key:\n                effective_api_key = os.environ.get(env_key)\n                if effective_api_key:\n                    logging.info(f\"Using API key from environment variable {env_key}\")\n                else:\n                    logging.warning(\n                        f\"API key for {llm_provider} not set in Valves or found in env var {env_key}. Proceeding without explicit key.\"\n                    )\n                    effective_api_key = \"\"  # Ensure it's an empty string if not found\n            else:\n                logging.info(\n                    f\"No specific environment variable mapped for provider '{llm_provider}'. Proceeding without explicit API key.\"\n                )\n                effective_api_key = \"\"\n        else:\n            logging.info(\"Using API key provided in Admin Valves.\")\n\n        # --- Prepare request data matching BrowserUI /run_deep_search API ---\n        # Ensure parameter order and types match the API documentation provided earlier\n        request_data_list = [\n            research_task,  # research_task (str)\n            float(max_search_iterations),  # max_search_iteration_input (float)\n            float(max_query_per_iter),  # max_query_per_iter_input (float)\n            llm_provider,  # llm_provider (Literal)\n            llm_model_name,  # llm_model_name (Literal)\n            float(llm_num_ctx),  # llm_num_ctx (float)\n            float(llm_temperature),  # llm_temperature (float)\n            llm_base_url,  # llm_base_url (str)\n            effective_api_key or \"\",  # llm_api_key (str) - Ensure empty string if None\n            use_vision,  # use_vision (bool)\n            False,  # use_own_browser (bool) - Let's default this to False for simplicity\n            headless,  # headless (bool)\n            \"\",  # chrome_cdp (str) - Default to empty\n        ]\n\n        request_payload = {\"data\": request_data_list}\n        # Use the Gradio API endpoint path\n        start_url = f\"{api_host}/gradio_api/call/run_deep_search\"\n\n        logging.info(f\"Posting to start URL: {start_url}\")\n        # Log parameters excluding sensitive API key\n        log_payload_data = {\n            f\"param_{i}\": v for i, v in enumerate(request_data_list) if i != 8\n        }  # Index 8 is the API key\n        logging.debug(f\"Request Payload Data (key omitted): {log_payload_data}\")\n\n        try:\n            # --- Step 1: Initiate the task ---\n            response = requests.post(\n                start_url, json=request_payload, timeout=request_timeout\n            )\n            response.raise_for_status()\n            response_json = response.json()\n            event_id = response_json.get(\"event_id\") or response_json.get(\n                \"session_hash\"\n            )  # Handle both possibilities\n\n            if not event_id:\n                logging.error(\n                    f\"Failed to get event_id/session_hash from BrowserUI response: {response_json}\"\n                )\n                error_msg = \"Error: Could not get task identifier from BrowserUI.\"\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": error_msg,\n                            \"done\": True,\n                            \"hidden\": False,\n                        },\n                    }\n                )\n                return error_msg  # Return error to LLM\n\n            logging.info(f\"Task initiated. Event ID/Session Hash: {event_id}\")\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": \"Task submitted to BrowserUI, waiting for progress...\",\n                        \"done\": False,\n                        \"hidden\": False,\n                    },\n                }\n            )\n\n            # --- Step 2: Connect to the streaming endpoint ---\n            stream_url = f\"{api_host}/gradio_api/queue/data?session_hash={event_id}\"\n            logging.info(f\"Connecting to stream URL: {stream_url}\")\n\n            final_result = \"\"\n            download_link = (\n                \"\"  # Not strictly needed for return, but can track if available\n            )\n\n            # Use a context manager for the streaming request\n            with requests.get(stream_url, stream=True, timeout=None) as stream_response:\n                stream_response.raise_for_status()\n                logging.info(\"Connected to BrowserUI event stream.\")\n\n                for line in stream_response.iter_lines():\n                    if not line:\n                        continue\n                    decoded_line = line.decode(\"utf-8\").strip()\n                    if decoded_line.startswith(\"data: \"):\n                        decoded_line = decoded_line[len(\"data: \") :].strip()\n                    if not decoded_line:\n                        continue\n\n                    try:\n                        msg = json.loads(decoded_line)\n                        msg_type = msg.get(\"msg\")\n                        logging.debug(f\"Received stream message type: {msg_type}\")\n\n                        status_description = None\n                        is_complete = False\n\n                        # Handle various message types for status updates\n                        if msg_type == \"estimation\":\n                            status_description = \"BrowserUI: Estimating queue time...\"\n                        elif msg_type == \"process_starts\":\n                            status_description = (\n                                \"BrowserUI: Research process starting...\"\n                            )\n                        elif msg_type == \"heartbeat\":\n                            continue  # Usually too noisy for status updates\n                        elif msg_type == \"progress\":\n                            status_description = (\n                                \"BrowserUI: Making progress...\"  # Generic progress\n                            )\n                        elif msg_type == \"process_generating\":\n                            status_description = \"BrowserUI: Generating final report...\"\n                        elif msg_type == \"log\":  # Optionally relay logs\n                            log_msg = msg.get(\"log\", \"Log message\")\n                            log_level = msg.get(\"level\", \"info\").lower()\n                            status_description = f\"BrowserUI Log ({log_level}): {log_msg[:100]}{'...' if len(log_msg)>100 else ''}\"\n                            # logging.info(f\"BrowserUI Log: {log_msg}\") # Log it internally too\n\n                        elif msg_type == \"process_completed\":\n                            is_complete = True\n                            status_description = \"BrowserUI: Task completed.\"\n                            output_data = msg.get(\"output\", {}).get(\"data\", [])\n                            if (\n                                output_data\n                                and isinstance(output_data, list)\n                                and len(output_data) > 0\n                            ):\n                                final_result = output_data[0]\n                                if len(output_data) > 1 and isinstance(\n                                    output_data[1], dict\n                                ):\n                                    # Check for download link if needed in the future\n                                    download_link = output_data[1].get(\"url\", \"\")\n                                    logging.info(\n                                        f\"Download link found: {download_link}\"\n                                    )\n                                logging.info(\"Final report received from BrowserUI.\")\n                            else:\n                                final_result = \"Error: BrowserUI completed but returned empty or invalid data.\"\n                                logging.error(\n                                    f\"Unexpected output format: {output_data}\"\n                                )\n                            break  # Exit loop on completion\n\n                        # Emit status update if description was set\n                        if status_description:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"status\",\n                                    \"data\": {\n                                        \"description\": status_description,\n                                        \"done\": False,\n                                        \"hidden\": False,\n                                    },\n                                }\n                            )\n\n                    except json.JSONDecodeError:\n                        logging.warning(\n                            f\"Could not decode JSON from stream line: '{decoded_line}'\"\n                        )\n                    except Exception as e:\n                        logging.error(\n                            f\"Error processing stream message: {msg if 'msg' in locals() else decoded_line} - Error: {e}\",\n                            exc_info=True,\n                        )\n                        final_result = f\"Error processing BrowserUI stream: {e}\"\n                        await __event_emitter__(\n                            {\n                                \"type\": \"status\",\n                                \"data\": {\n                                    \"description\": f\"Error: {final_result}\",\n                                    \"done\": True,\n                                    \"hidden\": False,\n                                },\n                            }\n                        )\n                        is_complete = True  # Treat as complete with error\n                        break  # Exit loop on error\n\n            # --- Final processing after stream closes or completes ---\n            if not final_result:\n                final_result = \"Error: Task finished, but no final report was captured from BrowserUI.\"\n                logging.warning(final_result)\n                await __event_emitter__(\n                    {\n                        \"type\": \"status\",\n                        \"data\": {\n                            \"description\": final_result,\n                            \"done\": True,\n                            \"hidden\": False,\n                        },\n                    }\n                )\n            else:\n                # Emit final \"completed\" status if not already done by error handling\n                if (\n                    not is_complete\n                ):  # Only if loop finished without break due to 'process_completed' or error\n                    await __event_emitter__(\n                        {\n                            \"type\": \"status\",\n                            \"data\": {\n                                \"description\": \"BrowserUI: Task completed successfully.\",\n                                \"done\": True,\n                                \"hidden\": True,\n                            },  # Hide final status\n                        }\n                    )\n\n            # --- Emit final message content (optional but good practice) ---\n            # This appears as a separate message in the chat after the LLM processes the return value\n            # You might choose *not* to emit a message here and just rely on the LLM using the return value.\n            # Let's comment it out for now, as the primary goal is to return the report to the LLM.\n            # await __event_emitter__({\n            #     \"type\": \"message\",\n            #     \"data\": { \"content\": f\"**BrowserUI Research Report:**\\n\\n{final_result}\" }\n            # })\n\n            # --- Return the final report string to the LLM ---\n            logging.info(\n                f\"Returning final result to LLM (length: {len(final_result)}).\"\n            )\n            return final_result\n\n        except requests.exceptions.Timeout:\n            error_message = f\"Error: Timeout connecting to BrowserUI API at {start_url} after {request_timeout}s.\"\n            logging.error(error_message)\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": error_message,\n                        \"done\": True,\n                        \"hidden\": False,\n                    },\n                }\n            )\n            return error_message\n        except requests.exceptions.RequestException as e:\n            error_message = (\n                f\"Error: Network error connecting to BrowserUI API at {api_host}: {e}\"\n            )\n            logging.error(error_message, exc_info=True)\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": error_message,\n                        \"done\": True,\n                        \"hidden\": False,\n                    },\n                }\n            )\n            return error_message  # Return error string to LLM\n        except Exception as e:\n            error_message = f\"An unexpected error occurred in the BrowserUI tool: {e}\"\n            logging.error(error_message, exc_info=True)\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"description\": error_message,\n                        \"done\": True,\n                        \"hidden\": False,\n                    },\n                }\n            )\n            return error_message  # Return error string to LLM\n\n\n# Example of how to potentially use citations if needed later:\n# async def emit_citation(self, content, title, url, __event_emitter__):\n#     await __event_emitter__(\n#         {\n#             \"type\": \"citation\",\n#             \"data\": {\n#                 \"document\": [content],\n#                 \"metadata\": [\n#                     {\n#                         \"date_accessed\": datetime.now().isoformat(),\n#                         \"source\": title,\n#                     }\n#                 ],\n#                 \"source\": {\"name\": title, \"url\": url},\n#             },\n#         }\n#     )\n","specs":[{"name":"perform_deep_research","description":"Performs deep research on a given topic using an external BrowserUI instance.","parameters":{"properties":{"research_task":{"description":"The detailed research task or question.","type":"string"}},"required":["research_task"],"type":"object"}}],"meta":{"description":"This tool performs local deep research in realtime using browser use web ui similarly to OPENAI DeepResearch","manifest":{"title":"BrowserUI Deep Research","author":"JAYADEEP V","author_url":"https://github.com/jaideepv # Or your repo if applicable","git_url":"https://github.com/open-webui/open-webui # Or your repo if applicable","description":"Performs deep web research using a BrowserUI (e.g., running on localhost:7788) instance via its Gradio API, providing streaming status updates.","required_open_webui_version":"0.4.0 # Requires event emitters and async tools","requirements":"requests","version":"0.1.0","license":"MIT"}},"access_control":{},"updated_at":1752332927,"created_at":1751826792},{"id":"story_element_generator_tool","user_id":"76bb3965-6767-4daa-810e-ae33b4628a48","name":"Story Element Generator Tool","content":"\"\"\"\ntitle: Story Element Generator Tool\ndescription: Generate a wide array of story elements, from individual components to comprehensive story outlines.\nauthor: @iamg30\nauthor_url: https://openwebui.com/u/iamg30/\nfunding_url: https://github.com/open-webui\nversion: 0.1.1\nlicense: MIT\n\"\"\"\n\nimport random\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional\n\n\nclass Tools:\n    class Valves(BaseModel):\n        CHARACTER_TRAITS: List[str] = Field(\n            default=[\n                \"brave\",\n                \"shy\",\n                \"intelligent\",\n                \"clumsy\",\n                \"charismatic\",\n                \"mysterious\",\n                \"loyal\",\n                \"ambitious\",\n                \"compassionate\",\n                \"arrogant\",\n                \"creative\",\n                \"patient\",\n                \"quirky\",\n                \"pessimistic\",\n                \"optimistic\",\n                \"sarcastic\",\n                \"determined\",\n                \"naive\",\n                \"wise\",\n                \"impulsive\",\n                \"methodical\",\n                \"eccentric\",\n                \"stoic\",\n                \"flamboyant\",\n            ],\n            description=\"List of character traits\",\n        )\n        OCCUPATIONS: List[str] = Field(\n            default=[\n                \"teacher\",\n                \"detective\",\n                \"chef\",\n                \"astronaut\",\n                \"artist\",\n                \"doctor\",\n                \"farmer\",\n                \"librarian\",\n                \"scientist\",\n                \"athlete\",\n                \"musician\",\n                \"entrepreneur\",\n                \"spy\",\n                \"archaeologist\",\n                \"pilot\",\n                \"hacker\",\n                \"diplomat\",\n                \"magician\",\n                \"journalist\",\n                \"bounty hunter\",\n                \"inventor\",\n                \"marine biologist\",\n                \"stunt performer\",\n                \"cryptozoologist\",\n            ],\n            description=\"List of occupations\",\n        )\n        SETTINGS: List[str] = Field(\n            default=[\n                \"medieval castle\",\n                \"futuristic city\",\n                \"tropical island\",\n                \"haunted house\",\n                \"underground bunker\",\n                \"space station\",\n                \"ancient ruins\",\n                \"bustling marketplace\",\n                \"quiet village\",\n                \"cyberpunk metropolis\",\n                \"enchanted forest\",\n                \"desert oasis\",\n                \"underwater city\",\n                \"steampunk airship\",\n                \"post-apocalyptic wasteland\",\n                \"floating sky kingdom\",\n                \"prehistoric jungle\",\n                \"arctic research station\",\n                \"dimensional nexus\",\n                \"virtual reality world\",\n            ],\n            description=\"List of story settings\",\n        )\n        PLOT_ELEMENTS: List[str] = Field(\n            default=[\n                \"unexpected inheritance\",\n                \"mysterious disappearance\",\n                \"time travel accident\",\n                \"alien invasion\",\n                \"forbidden romance\",\n                \"hidden treasure\",\n                \"epic quest\",\n                \"political intrigue\",\n                \"magical transformation\",\n                \"natural disaster\",\n                \"artificial intelligence rebellion\",\n                \"parallel universe discovery\",\n                \"ancient prophecy fulfillment\",\n                \"memory manipulation conspiracy\",\n                \"supernatural ability awakening\",\n                \"interspecies diplomatic crisis\",\n                \"reality-altering experiment\",\n                \"cosmic entity encounter\",\n                \"historical figure resurrection\",\n                \"sentient planet exploration\",\n            ],\n            description=\"List of plot elements\",\n        )\n        GENRES: List[str] = Field(\n            default=[\n                \"science fiction\",\n                \"fantasy\",\n                \"mystery\",\n                \"romance\",\n                \"thriller\",\n                \"horror\",\n                \"adventure\",\n                \"historical fiction\",\n                \"dystopian\",\n                \"comedy\",\n                \"drama\",\n                \"action\",\n                \"magical realism\",\n                \"cyberpunk\",\n                \"steampunk\",\n                \"supernatural\",\n                \"psychological\",\n                \"crime\",\n                \"post-apocalyptic\",\n                \"alternate history\",\n            ],\n            description=\"List of story genres\",\n        )\n        THEMES: List[str] = Field(\n            default=[\n                \"redemption\",\n                \"coming of age\",\n                \"power corrupts\",\n                \"love conquers all\",\n                \"man vs. nature\",\n                \"technology gone wrong\",\n                \"identity crisis\",\n                \"good vs. evil\",\n                \"sacrifice for greater good\",\n                \"the human condition\",\n                \"moral ambiguity\",\n                \"cycle of violence\",\n                \"triumph over adversity\",\n                \"loss of innocence\",\n                \"beauty in imperfection\",\n                \"consequences of ambition\",\n                \"illusion vs. reality\",\n                \"power of friendship\",\n                \"nature of consciousness\",\n                \"impact of choices\",\n            ],\n            description=\"List of story themes\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n\n    def generate_character(self, trait_count: int = 2) -> str:\n        \"\"\"\n        Generate a character with random traits and occupation.\n        :param trait_count: Number of traits to assign to the character.\n        :return: A JSON string containing character details.\n        \"\"\"\n        traits = random.sample(\n            self.valves.CHARACTER_TRAITS,\n            min(trait_count, len(self.valves.CHARACTER_TRAITS)),\n        )\n        occupation = random.choice(self.valves.OCCUPATIONS)\n\n        character = {\"traits\": traits, \"occupation\": occupation}\n\n        return json.dumps(character, indent=2)\n\n    def generate_setting(self) -> str:\n        \"\"\"\n        Generate a random story setting.\n        :return: A JSON string containing setting details.\n        \"\"\"\n        setting = random.choice(self.valves.SETTINGS)\n        return json.dumps({\"setting\": setting}, indent=2)\n\n    def generate_plot_point(self) -> str:\n        \"\"\"\n        Generate a random plot point or story element.\n        :return: A JSON string containing a plot point.\n        \"\"\"\n        plot_point = random.choice(self.valves.PLOT_ELEMENTS)\n        return json.dumps({\"plot_point\": plot_point}, indent=2)\n\n    def generate_genre(self) -> str:\n        \"\"\"\n        Generate a random story genre.\n        :return: A JSON string containing a genre.\n        \"\"\"\n        genre = random.choice(self.valves.GENRES)\n        return json.dumps({\"genre\": genre}, indent=2)\n\n    def generate_theme(self) -> str:\n        \"\"\"\n        Generate a random story theme.\n        :return: A JSON string containing a theme.\n        \"\"\"\n        theme = random.choice(self.valves.THEMES)\n        return json.dumps({\"theme\": theme}, indent=2)\n\n    def generate_story_starter(\n        self,\n        trait_count: int = 2,\n        include_genre: bool = True,\n        include_theme: bool = True,\n    ) -> str:\n        \"\"\"\n        Generate a complete story starter with character, setting, plot point, and optionally genre and theme.\n        :param trait_count: Number of traits to assign to the character.\n        :param include_genre: Whether to include a genre in the story starter.\n        :param include_theme: Whether to include a theme in the story starter.\n        :return: A JSON string containing a story starter.\n        \"\"\"\n        character = json.loads(self.generate_character(trait_count))\n        setting = json.loads(self.generate_setting())\n        plot_point = json.loads(self.generate_plot_point())\n\n        story_starter = {\n            \"character\": character,\n            \"setting\": setting[\"setting\"],\n            \"plot_point\": plot_point[\"plot_point\"],\n        }\n\n        if include_genre:\n            genre = json.loads(self.generate_genre())\n            story_starter[\"genre\"] = genre[\"genre\"]\n\n        if include_theme:\n            theme = json.loads(self.generate_theme())\n            story_starter[\"theme\"] = theme[\"theme\"]\n\n        return json.dumps(story_starter, indent=2)\n\n    def add_custom_element(self, element_type: str, new_element: str) -> str:\n        \"\"\"\n        Add a custom element to one of the predefined lists.\n        :param element_type: The type of element to add (trait, occupation, setting, plot_element, genre, or theme).\n        :param new_element: The new element to add.\n        :return: A JSON string confirming the addition or an error message.\n        \"\"\"\n        element_lists = {\n            \"trait\": self.valves.CHARACTER_TRAITS,\n            \"occupation\": self.valves.OCCUPATIONS,\n            \"setting\": self.valves.SETTINGS,\n            \"plot_element\": self.valves.PLOT_ELEMENTS,\n            \"genre\": self.valves.GENRES,\n            \"theme\": self.valves.THEMES,\n        }\n\n        if element_type in element_lists:\n            if new_element not in element_lists[element_type]:\n                element_lists[element_type].append(new_element)\n                return json.dumps(\n                    {\"message\": f\"Added '{new_element}' to {element_type}s.\"}, indent=2\n                )\n            else:\n                return json.dumps(\n                    {\"error\": f\"'{new_element}' already exists in {element_type}s.\"},\n                    indent=2,\n                )\n        else:\n            return json.dumps(\n                {\n                    \"error\": \"Invalid element type. Choose from: trait, occupation, setting, plot_element, genre, or theme.\"\n                },\n                indent=2,\n            )\n\n    def generate_story_outline(self, num_scenes: int = 3) -> str:\n        \"\"\"\n        Generate a basic story outline with multiple scenes.\n        :param num_scenes: Number of scenes to generate for the outline.\n        :return: A JSON string containing a story outline.\n        \"\"\"\n        story_starter = json.loads(self.generate_story_starter())\n        scenes = []\n\n        for i in range(num_scenes):\n            scene = {\n                \"setting\": json.loads(self.generate_setting())[\"setting\"],\n                \"plot_point\": json.loads(self.generate_plot_point())[\"plot_point\"],\n            }\n            scenes.append(scene)\n\n        outline = {\n            \"main_character\": story_starter[\"character\"],\n            \"genre\": story_starter.get(\"genre\", \"Not specified\"),\n            \"theme\": story_starter.get(\"theme\", \"Not specified\"),\n            \"opening_scene\": {\n                \"setting\": story_starter[\"setting\"],\n                \"plot_point\": story_starter[\"plot_point\"],\n            },\n            \"additional_scenes\": scenes,\n        }\n\n        return json.dumps(outline, indent=2)\n\n    def generate_character_relationship(self) -> str:\n        \"\"\"\n        Generate a random character relationship.\n        :return: A JSON string containing a character relationship.\n        \"\"\"\n        relationships = [\n            \"siblings\",\n            \"best friends\",\n            \"rivals\",\n            \"mentor and student\",\n            \"lovers\",\n            \"partners in crime\",\n            \"parent and child\",\n            \"nemeses\",\n            \"coworkers\",\n            \"childhood friends\",\n            \"teacher and pupil\",\n            \"hero and sidekick\",\n            \"captor and prisoner\",\n            \"ruler and subject\",\n            \"guardian and ward\",\n        ]\n        relationship = random.choice(relationships)\n        return json.dumps({\"relationship\": relationship}, indent=2)\n\n    def generate_conflict(self) -> str:\n        \"\"\"\n        Generate a random conflict for the story.\n        :return: A JSON string containing a conflict.\n        \"\"\"\n        conflicts = [\n            \"internal struggle\",\n            \"person vs. person\",\n            \"person vs. nature\",\n            \"person vs. society\",\n            \"person vs. technology\",\n            \"person vs. supernatural\",\n            \"moral dilemma\",\n            \"unrequited love\",\n            \"betrayal\",\n            \"race against time\",\n            \"clash of ideologies\",\n            \"power struggle\",\n            \"identity crisis\",\n            \"revenge plot\",\n            \"quest for redemption\",\n        ]\n        conflict = random.choice(conflicts)\n        return json.dumps({\"conflict\": conflict}, indent=2)\n\n    def generate_complete_story_concept(self) -> str:\n        \"\"\"\n        Generate a complete story concept with multiple elements.\n        :return: A JSON string containing a comprehensive story concept.\n        \"\"\"\n        outline = json.loads(self.generate_story_outline(num_scenes=3))\n        secondary_character = json.loads(self.generate_character())\n        relationship = json.loads(self.generate_character_relationship())\n        conflict = json.loads(self.generate_conflict())\n\n        story_concept = {\n            \"main_character\": outline[\"main_character\"],\n            \"secondary_character\": secondary_character,\n            \"character_relationship\": relationship[\"relationship\"],\n            \"genre\": outline[\"genre\"],\n            \"theme\": outline[\"theme\"],\n            \"central_conflict\": conflict[\"conflict\"],\n            \"setting\": outline[\"opening_scene\"][\"setting\"],\n            \"plot_outline\": [outline[\"opening_scene\"]] + outline[\"additional_scenes\"],\n        }\n\n        return json.dumps(story_concept, indent=2)\n","specs":[{"name":"add_custom_element","description":"Add a custom element to one of the predefined lists.","parameters":{"properties":{"element_type":{"description":"The type of element to add (trait, occupation, setting, plot_element, genre, or theme).","type":"string"},"new_element":{"description":"The new element to add.","type":"string"}},"required":["element_type","new_element"],"type":"object"}},{"name":"generate_character","description":"Generate a character with random traits and occupation.","parameters":{"properties":{"trait_count":{"default":2,"description":"Number of traits to assign to the character.","type":"integer"}},"type":"object"}},{"name":"generate_character_relationship","description":"Generate a random character relationship.","parameters":{"properties":{},"type":"object"}},{"name":"generate_complete_story_concept","description":"Generate a complete story concept with multiple elements.","parameters":{"properties":{},"type":"object"}},{"name":"generate_conflict","description":"Generate a random conflict for the story.","parameters":{"properties":{},"type":"object"}},{"name":"generate_genre","description":"Generate a random story genre.","parameters":{"properties":{},"type":"object"}},{"name":"generate_plot_point","description":"Generate a random plot point or story element.","parameters":{"properties":{},"type":"object"}},{"name":"generate_setting","description":"Generate a random story setting.","parameters":{"properties":{},"type":"object"}},{"name":"generate_story_outline","description":"Generate a basic story outline with multiple scenes.","parameters":{"properties":{"num_scenes":{"default":3,"description":"Number of scenes to generate for the outline.","type":"integer"}},"type":"object"}},{"name":"generate_story_starter","description":"Generate a complete story starter with character, setting, plot point, and optionally genre and theme.","parameters":{"properties":{"trait_count":{"default":2,"description":"Number of traits to assign to the character.","type":"integer"},"include_genre":{"default":true,"description":"Whether to include a genre in the story starter.","type":"boolean"},"include_theme":{"default":true,"description":"Whether to include a theme in the story starter.","type":"boolean"}},"type":"object"}},{"name":"generate_theme","description":"Generate a random story theme.","parameters":{"properties":{},"type":"object"}}],"meta":{"description":"This tool is designed to generate a wide array of story elements, from individual components to comprehensive story outlines.","manifest":{"title":"Story Element Generator Tool","description":"Generate a wide array of story elements, from individual components to comprehensive story outlines.","author":"@iamg30","author_url":"https://openwebui.com/u/iamg30/","funding_url":"https://github.com/open-webui","version":"0.1.1","license":"MIT"}},"access_control":{},"updated_at":1752332470,"created_at":1751826956},{"id":"word_document_generator","user_id":"76bb3965-6767-4daa-810e-ae33b4628a48","name":"Word Document Generator","content":"import os\nimport time\nimport traceback\nimport aiohttp\nfrom docx import Document\n\n# 🔐 Your actual API key from OpenWebUI\nAPI_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjNiOWYwMGZhLTZkYWYtNDk3My1hZGRmLWEwNmJlMzk4Zjk2YiJ9.JK-d1JKXsDCLfLOMJ3SWpRIKC5CLeGpRn2VV0h0HCd0\"\n\n# 🌐 Your domain hosting Ollama WebUI\nPUBLIC_DOWNLOAD_DOMAIN = \"https://socialgarden-ollama-web.vo0egb.easypanel.host\"\n\n\nclass Tools:\n    def __init__(self):\n        self.file_handler = True  # OpenWebUI expects this flag for file-returning tools\n\n    async def generate_word_doc(self, title: str, content: str) -> dict:\n        safe = title.replace(\" \", \"_\").replace(\"/\", \"_\")\n        filename = f\"{safe}_{int(time.time())}.docx\"\n        local_path = f\"/tmp/{filename}\"\n\n        try:\n            doc = Document()\n            doc.add_paragraph(title)\n            doc.add_paragraph(content)\n            doc.save(local_path)\n        except Exception as e:\n            return {\"error\": f\"❌ Failed to generate DOCX: {e}\"}\n\n        headers = {\n            \"Authorization\": f\"Bearer {API_KEY}\",\n            \"Accept\": \"application/json\",\n        }\n\n        try:\n            form = aiohttp.FormData()\n            form.add_field(\n                \"file\",\n                open(local_path, \"rb\"),\n                filename=filename,\n                content_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n            )\n\n            async with aiohttp.ClientSession(\n                timeout=aiohttp.ClientTimeout(total=30)\n            ) as session:\n                async with session.post(\n                    f\"{PUBLIC_DOWNLOAD_DOMAIN}/api/v1/files/\",\n                    headers=headers,\n                    data=form,\n                ) as resp:\n                    status = resp.status\n                    text = await resp.text()\n\n            os.remove(local_path)  # Cleanup\n\n            if status < 200 or status >= 300:\n                return {\"error\": f\"❌ HTTP {status}\", \"raw_response\": text}\n\n            try:\n                data = await resp.json()\n            except Exception:\n                return {\n                    \"error\": \"❌ Invalid JSON\",\n                    \"status_code\": status,\n                    \"raw_response\": text,\n                }\n\n            file_id = data.get(\"id\") or data.get(\"uuid\") or data.get(\"file_id\")\n            if not file_id:\n                return {\"error\": \"❌ No file ID in response\", \"json\": data}\n\n            download_url = f\"{PUBLIC_DOWNLOAD_DOMAIN}/api/v1/files/{file_id}/content\"\n            return {\"url\": download_url}\n\n        except Exception as e:\n            print(traceback.format_exc())\n            return {\"error\": f\"❌ Upload failed: {e}\"}\n","specs":[{"name":"generate_word_doc","description":"","parameters":{"properties":{"title":{"type":"string"},"content":{"type":"string"}},"required":["title","content"],"type":"object"}}],"meta":{"description":"Generates a word document from a given prompt. Provides download url for it in the chat. ","manifest":{}},"access_control":{},"updated_at":1752205125,"created_at":1751986792},{"id":"calculator","user_id":"76bb3965-6767-4daa-810e-ae33b4628a48","name":"Calculator","content":"\"\"\"\ntitle: Calculator\nauthor: open-webui\nauthor_url: https://github.com/open-webui\nfunding_url: https://github.com/open-webui\nversion: 0.1.0\n\"\"\"\n\nimport os\nimport requests\nfrom datetime import datetime\n\n\nclass Tools:\n    def __init__(self):\n        pass\n\n    # Add your custom tools using pure Python code here, make sure to add type hints\n    # Use Sphinx-style docstrings to document your tools, they will be used for generating tools specifications\n    # Please refer to function_calling_filter_pipeline.py file from pipelines project for an example\n\n    def calculator(self, equation: str) -> str:\n        \"\"\"\n        Calculate the result of an equation.\n        :param equation: The equation to calculate.\n        \"\"\"\n\n        # Avoid using eval in production code\n        # https://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html\n        try:\n            result = eval(equation)\n            return f\"{equation} = {result}\"\n        except Exception as e:\n            print(e)\n            return \"Invalid equation\"\n","specs":[{"name":"calculator","description":"Calculate the result of an equation.","parameters":{"properties":{"equation":{"description":"The equation to calculate.","type":"string"}},"required":["equation"],"type":"object"}}],"meta":{"description":"This tool is an example implementation of a  custom calculator tool for evaluating mathematical equations in Python.","manifest":{"title":"Calculator","author":"open-webui","author_url":"https://github.com/open-webui","funding_url":"https://github.com/open-webui","version":"0.1.0"}},"access_control":{},"updated_at":1752204818,"created_at":1751826701},{"id":"web_search_and_scraping_with_searxng_and_docling","user_id":"76bb3965-6767-4daa-810e-ae33b4628a48","name":"Web Search and scraping with Searxng and Docling","content":"\"\"\"\ntitle: Searxng Search and Docling Scraper\nauthor: Kemon\ndescription: This tool uses searxng to search the web and docling to scrape the webpages\nrequired_open_webui_version: 0.6.0\nrequirements:\nversion: 0.0.2\nlicence: MIT\n\"\"\"\n\nimport requests\nfrom datetime import date, datetime\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Callable, Awaitable\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Awaitable[None]] = None):\n        self.event_emitter = event_emitter\n\n    async def progress_update(self, description):\n        await self.emit(description)\n\n    async def error_update(self, description):\n        await self.emit(description, \"error\", True)\n\n    async def success_update(self, description):\n        await self.emit(description, \"success\", True)\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n    async def citation(self, document, url, name=None):\n        if self.event_emitter:\n            if name == None:\n                name = url\n\n            await self.event_emitter(\n                {\n                    \"type\": \"citation\",\n                    \"data\": {\n                        \"document\": [document],\n                        \"metadata\": [\n                            {\n                                \"date_accessed\": datetime.now().isoformat(),\n                                \"source\": url,\n                            }\n                        ],\n                        \"source\": {\"name\": name, \"url\": url},\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        DOCLING_URL: str = Field(\n            default=\"http://host.docker.internal:5001\",\n            description=\"Docling URL\",\n        )\n        DOCLING_TIMEOUT: float = Field(\n            default=5,\n            description=\"Docling URL Timeout\",\n        )\n        SEARXNG_URL: str = Field(\n            default=\"http://host.docker.internal:8080\",\n            description=\"SearXNG URL\",\n        )\n        SEARXNG_SCORE_FILTER: float = Field(\n            default=1,\n            description=\"SearXNG Score Filter\",\n        )\n        SEARXNG_SCORE_RESULT: int = Field(\n            default=5,\n            description=\"SearXNG Score Result\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.citation = False\n        pass\n\n    async def docling(\n        self,\n        urls: list,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n    ) -> str:\n        \"\"\"\n        Web scrape the website provided and get the content of it.\n        :param urls: List of urls.\n        :return: Return relevant context.\n        \"\"\"\n\n        docling_url = self.valves.DOCLING_URL\n        docling_timeout = self.valves.DOCLING_TIMEOUT\n\n        emitter = EventEmitter(__event_emitter__)\n\n        await emitter.progress_update(\"Scraping\")\n\n        api_url = f\"{docling_url}/v1alpha/convert/source\"\n        headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n\n        all_responses = []\n\n        for url in urls:\n            # add https to url if it does not start with http or https\n            if not url.startswith(\"http\") and not url.startswith(\"https\"):\n                url = f\"https://{url}\"\n\n            data = {\n                \"options\": {\"to_formats\": [\"md\", \"doctags\"]},\n                \"http_sources\": [{\"url\": url}],\n            }\n\n            if url.endswith(\".pdf\") or url.endswith(\".docx\"):\n                docling_timeout = docling_timeout * 2\n\n            try:\n                await emitter.progress_update(f\"Scraping {url}\")\n                response = requests.post(\n                    api_url, headers=headers, json=data, timeout=docling_timeout\n                )\n                await emitter.success_update(\"Scraping complete\")\n\n            except requests.exceptions.Timeout:\n                await emitter.error_update(f\"Timeout occurred for {url}\")\n                continue\n\n            except requests.exceptions.RequestException as e:\n                await emitter.error_update(f\"{e}\")\n                return f\"{e}\"\n\n            if response.status_code == 200:\n\n                data = response.json()\n\n                # Extract md_content and md_doctags from the JSON object\n                md_content = data[\"document\"][\"md_content\"]\n                md_doctags = data[\"document\"][\"doctags_content\"]\n\n                # Use a ternary operator to set the 'content' variable\n                content = md_content if md_content != \"\" else md_doctags\n\n                all_responses.append(f\"<url:{url}>\\n{content}\\n</url:{url}>\\n\")\n\n                await emitter.citation(content, url)\n\n            else:\n                await emitter.error_update(f\"Error fetching {url}\")\n\n        return \"\".join(all_responses)\n\n    async def searxng(\n        self, search_query: str, __event_emitter__: Callable[[dict], Awaitable[None]]\n    ) -> str:\n        \"\"\"\n        Search the web and get the content of the relevant pages. Search for unknown knowledge, news, info, public contact info, weather, etc.\n        :param query: Web Query used in search engine.\n        :return: Provide the answer to the question as promptly as possible.\n        \"\"\"\n        searxng_url = self.valves.SEARXNG_URL\n        searxng_score_filter = self.valves.SEARXNG_SCORE_FILTER\n        searxng_score_result = self.valves.SEARXNG_SCORE_RESULT\n\n        emitter = EventEmitter(__event_emitter__)\n\n        api_url = f\"{searxng_url}/search?q={search_query}&format=json&safesearch=0\"\n        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n\n        try:\n            await emitter.progress_update(\"Searching\")\n            response = requests.get(api_url, headers=headers)\n\n        except requests.exceptions.Timeout:\n            await emitter.error_update(f\"Timeout occurred for {url}\")\n            return f\"Timeout occurred for {url}\"\n\n        except requests.exceptions.RequestException as e:\n            await emitter.error_update(f\"{e}\")\n            return f\"{e}\"\n\n        await emitter.progress_update(\"Search Complete\")\n\n        if response.status_code == 200:\n            data = response.json()\n\n            await emitter.progress_update(\"Generating Response\")\n\n            data[\"results\"] = [\n                result[\"url\"]\n                for result in data[\"results\"]\n                if result[\"score\"] > searxng_score_filter\n            ]\n\n            results = await self.docling(\n                data[\"results\"][:searxng_score_result], __event_emitter__\n            )\n\n            await emitter.success_update(f\"Complete! Search Query: {search_query}\")\n\n            return results\n\n        else:\n            await emitter.error_update(\n                f\"Failed to fetch results from SearXNG: {response.status_code}\"\n            )\n            return [f\"Failed to fetch results from SearXNG: {response.status_code}\"]\n","specs":[{"name":"docling","description":"Web scrape the website provided and get the content of it.","parameters":{"properties":{"urls":{"description":"List of urls.","items":{},"type":"array"}},"required":["urls"],"type":"object"}},{"name":"searxng","description":"Search the web and get the content of the relevant pages. Search for unknown knowledge, news, info, public contact info, weather, etc.","parameters":{"properties":{"search_query":{"type":"string"}},"required":["search_query"],"type":"object"}}],"meta":{"description":"This tool uses searxng to search the web and docling to scrape the webpages","manifest":{"title":"Searxng Search and Docling Scraper","author":"Kemon","description":"This tool uses searxng to search the web and docling to scrape the webpages","required_open_webui_version":"0.6.0","requirements":"","version":"0.0.2","licence":"MIT"}},"access_control":{},"updated_at":1751986528,"created_at":1751114753},{"id":"bbc_news_feeds","user_id":"76bb3965-6767-4daa-810e-ae33b4628a48","name":"BBC News Feeds","content":"\"\"\"\ntitle: BBC News Feeds\nauthor: @nathanwindisch\nauthor_url: https://github.com/nathanwindisch\nfunding_url: https://www.patreon.com/NathanWindisch\nversion: 0.0.9\nchangelog:\n- 0.0.1 - Initial upload to openwebui community.\n- 0.0.2 - Modified formatting slightly.\n- 0.0.3 - Added tool docstring, and this changelog.\n- 0.0.4 - Added funding_url to docstring.\n- 0.0.5 - Updated get_bbc_news_feed function to use a default for\n          the ArticleType, and updated it's docstring to include\n          a list of the possible types, to assist the LLM's query.\n- 0.0.6 - Added event emitter to the get_bbc_news_feed function,\n          to provide status updates to the user as the function\n          executes. Also wrapped the function in a try/catch, to\n          handle any exceptions that may occur during execution.\n- 0.0.7 - Fixed a major bug where the type was not being casted\n          to the ArticleType enum, causing the get_uri function\n          to not be called correctly.\n- 0.0.8 - Updated the ArticleType parameter docstring to make it\n          mandatory, and for to contain the full names of the\n          'world/' types rather than the abbreviations.\n- 0.0.9 - Created a new function, get_bbc_news_content, which\n          retrieves the article text content of a BBC News link,\n          given it's URI.\n\"\"\"\n\nimport re\nimport json\nimport requests\nimport xml.etree.ElementTree as ElementTree\nfrom typing import Awaitable, Callable\nfrom pydantic import BaseModel\nfrom enum import Enum\nfrom bs4 import BeautifulSoup\n\n\nclass ArticleType(Enum):\n    top_stories = \"\"\n    world = \"world\"\n    uk = \"uk\"\n    business = \"business\"\n    politics = \"politics\"\n    health = \"health\"\n    education = \"education\"\n    science_and_environment = \"science_and_environment\"\n    technology = \"technology\"\n    entertainment_and_arts = \"entertainment_and_arts\"\n    england = \"england\"\n    northern_ireland = \"northern_ireland\"\n    scotland = \"scotland\"\n    wales = \"wales\"\n    africa = \"world/africa\"\n    asia = \"world/asia\"\n    australia = \"world/australia\"\n    europe = \"world/europe\"\n    latin_america = \"world/latin_america\"\n    middle_east = \"world/middle_east\"\n    us_and_canada = \"world/us_and_canada\"\n\n    def get_name(self) -> str:\n        return self.name.replace(\"_\", \" \").title()\n\n    def get_uri(self) -> str:\n        return (\n            f\"https://feeds.bbci.co.uk/news/{self.value}/rss.xml\"\n            if self.name != \"top_stories\"\n            else \"https://feeds.bbci.co.uk/news/rss.xml\"\n        )\n\n\n# Regex to match a BBC News article URI.\n# Details:\n#  - Must use http or https.\n#  - Must be a bbc.com or bbc.co.uk domain.\n#  - Must be a news article or video.\n#  - Must have a valid ID (alphanumeric characters).\nURI_REGEX = re.compile(\n    \"^(https?:\\/\\/)(www\\.)?bbc\\.(com|co\\.uk)\\/news\\/(articles|videos)\\/\\w+$\"\n)\n\n\nclass Tools:\n    def __init__(self):\n        pass\n\n    class UserValves(BaseModel):\n        pass\n\n    async def get_bbc_news_feed(\n        self,\n        type: ArticleType,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        __user__: dict = {},\n    ) -> str:\n        \"\"\"\n        Get the latest news from the BBC, as an array of JSON objects with a title, description, link, and published date.\n        :param type: The type of news to get. It can be any of the ArticleType enum values (world, uk, business, politics, health, education, science_and_environment, technology, entertainment_and_arts, england, northern_ireland, scotland, wales, world/africa, world/asia, world/australia, world/europe, world/latin_america, world/middle_east, world/us_and_canada).\n        :return: A list of news items or an error message.\n        \"\"\"\n        await __event_emitter__(\n            {\n                \"data\": {\n                    \"description\": f\"Starting BBC News Feed retrieval for articles in the '{type.get_name()}' category...\",\n                    \"status\": \"in_progress\",\n                    \"done\": False,\n                },\n                \"type\": \"status\",\n            }\n        )\n        type = ArticleType(\n            type\n        )  # Enforce the type (it seems to get dropped by openwebui...)\n        output = []\n        try:\n            response = requests.get(type.get_uri())\n            if not response.ok:\n                return f\"Error: '{type}' ({type.get_uri()}) not found ({response.status_code})\"\n            root = ElementTree.fromstring(response.content)\n            for item in root.iter(\"item\"):\n                output.append(\n                    {\n                        \"title\": item.find(\"title\").text,\n                        \"description\": item.find(\"description\").text,\n                        \"link\": item.find(\"link\").text,\n                        \"published\": item.find(\"pubDate\").text,\n                    }\n                )\n            await __event_emitter__(\n                {\n                    \"data\": {\n                        \"description\": f\"Retrieved {len(output)} news items from BBC News Feed for articles in the '{type.get_name()}' category.\",\n                        \"status\": \"complete\",\n                        \"done\": True,\n                    },\n                    \"type\": \"status\",\n                }\n            )\n        except Exception as e:\n            await __event_emitter__(\n                {\n                    \"data\": {\n                        \"description\": f\"Failed to retrieved any news items from BBC News Feed for articles in the '{type.get_name()}' ({type.get_uri()}) category: {e}.\",\n                        \"status\": \"complete\",\n                        \"done\": True,\n                    },\n                    \"type\": \"status\",\n                }\n            )\n            return f\"Error: {e}\"\n\n        return json.dumps(output)\n\n    async def get_bbc_news_content(\n        self,\n        uri: str,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        __user__: dict = {},\n    ) -> str:\n        \"\"\"\n        Get the content of a news article from the BBC.\n        :param uri: The URI of the article to get the content of, which should start with https://bbc.com/news or https://bbc.co.uk/news.\n        :return: The content of the article or an error message.\n        \"\"\"\n        await __event_emitter__(\n            {\n                \"data\": {\n                    \"description\": f\"Starting BBC News Article retrieval from '{uri}'...\",\n                    \"status\": \"in_progress\",\n                    \"done\": False,\n                },\n                \"type\": \"status\",\n            }\n        )\n\n        if uri == \"\":\n            await __event_emitter__(\n                {\n                    \"data\": {\n                        \"description\": f\"Error: No URI provided.\",\n                        \"status\": \"complete\",\n                        \"done\": True,\n                    },\n                    \"type\": \"status\",\n                }\n            )\n            return \"Error: No URI provided\"\n\n        if not re.match(URI_REGEX, uri):\n            await __event_emitter__(\n                {\n                    \"data\": {\n                        \"description\": f\"Error: URI must be a BBC News article.\",\n                        \"status\": \"complete\",\n                        \"done\": True,\n                    },\n                    \"type\": \"status\",\n                }\n            )\n            return \"Error: URI must be a BBC News article.\"\n\n        content = \"\"\n        try:\n            response = requests.get(uri)\n            if not response.ok:\n                return f\"Error: '{uri}' not found ({response.status_code})\"\n            article = BeautifulSoup(response.content, \"html.parser\").find(\"article\")\n            if article is None:\n                await __event_emitter__(\n                    {\n                        \"data\": {\n                            \"description\": f\"Failed to retrieve BBC News Article content from '{uri}': Article content not found.\",\n                            \"status\": \"complete\",\n                            \"done\": True,\n                        },\n                        \"type\": \"status\",\n                    }\n                )\n                return f\"Error: Article content for {uri} not found.\"\n\n            paragraphs = article.find_all(\"p\")\n            for paragraph in paragraphs:\n                content += f\"{paragraph.text}\\n\"\n            await __event_emitter__(\n                {\n                    \"data\": {\n                        \"description\": f\"Retrieved BBC News Article content from '{uri}' ({len(content)} characters).\",\n                        \"status\": \"complete\",\n                        \"done\": True,\n                    },\n                    \"type\": \"status\",\n                }\n            )\n        except Exception as e:\n            await __event_emitter__(\n                {\n                    \"data\": {\n                        \"description\": f\"Failed to retrieve BBC News Article content from '{uri}': {e}.\",\n                        \"status\": \"complete\",\n                        \"done\": True,\n                    },\n                    \"type\": \"status\",\n                }\n            )\n            return f\"Error: {e}\"\n\n        return content\n","specs":[{"name":"get_bbc_news_content","description":"Get the content of a news article from the BBC.","parameters":{"properties":{"uri":{"description":"The URI of the article to get the content of, which should start with https://bbc.com/news or https://bbc.co.uk/news.","type":"string"}},"required":["uri"],"type":"object"}},{"name":"get_bbc_news_feed","description":"Get the latest news from the BBC, as an array of JSON objects with a title, description, link, and published date.","parameters":{"properties":{"type":{"enum":["","world","uk","business","politics","health","education","science_and_environment","technology","entertainment_and_arts","england","northern_ireland","scotland","wales","world/africa","world/asia","world/australia","world/europe","world/latin_america","world/middle_east","world/us_and_canada"],"type":"string"}},"required":["type"],"type":"object"}}],"meta":{"description":"Gets articles from BBC News' RSS feeds","manifest":{"title":"BBC News Feeds","author":"@nathanwindisch","author_url":"https://github.com/nathanwindisch","funding_url":"https://www.patreon.com/NathanWindisch","version":"0.0.9","changelog":""}},"access_control":{},"updated_at":1751826792,"created_at":1751826792}]